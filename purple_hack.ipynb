{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Train_Yolo_v8_dataset.zip -d dataset_train"
      ],
      "metadata": {
        "id": "S-T2jjPQC_C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-NxbIdeItFw",
        "outputId": "7d06ba26-b326-47db-cca2-95010dbdad05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.88)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8l.pt')\n",
        "\n",
        "results = model.train(\n",
        "    data='/content/dataset_train/one/dataset.yaml',\n",
        "    epochs=70,\n",
        "    batch=16,\n",
        "    imgsz=640,\n",
        "    split=0.8,\n",
        "    name='Avito_Hack_v4',\n",
        "    patience=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrcJUJD5KNt9",
        "outputId": "a97c68a1-79b7-4690-d40b-41cb88e23025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.88 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/content/dataset_train/one/dataset.yaml, epochs=70, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=Avito_Hack_v4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=0.8, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/Avito_Hack_v4\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5585884  ultralytics.nn.modules.head.Detect           [4, [256, 512, 512]]          \n",
            "Model summary: 209 layers, 43,632,924 parameters, 43,632,908 gradients, 165.4 GFLOPs\n",
            "\n",
            "Transferred 589/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/Avito_Hack_v4', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset_train/one/labels.cache... 1010 images, 0 backgrounds, 17 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1010/1010 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/0226fc21-36455397480.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/054dcec8-29609478060.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/2c361020-36455537468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/429e07e5-36455430499.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0013]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/44d77da3-36455523193.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0029]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/54df5495-29615489265.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/59802cc3-36455544135.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/5c107aed-29615858349.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/7986cf2f-36455602280.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/94de3322-36455580468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/a6b57513-36455613018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ce8dca4a-27751638538.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.001]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/d199e7fd-27852669465.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3a8a11d-36455444159.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3c51271-36455653392.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e4a25f32-36455511260.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ece839a8-36429516961.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset_train/one/labels.cache... 1010 images, 0 backgrounds, 17 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1010/1010 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/0226fc21-36455397480.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/054dcec8-29609478060.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/2c361020-36455537468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/429e07e5-36455430499.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0013]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/44d77da3-36455523193.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0029]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/54df5495-29615489265.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/59802cc3-36455544135.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/5c107aed-29615858349.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/7986cf2f-36455602280.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/94de3322-36455580468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/a6b57513-36455613018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ce8dca4a-27751638538.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.001]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/d199e7fd-27852669465.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3a8a11d-36455444159.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3c51271-36455653392.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e4a25f32-36455511260.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ece839a8-36429516961.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/Avito_Hack_v4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/Avito_Hack_v4\u001b[0m\n",
            "Starting training for 70 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/70      9.62G      0.781      2.027      1.304          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:51<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217     0.0652      0.331     0.0487     0.0203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/70      12.1G      1.238      2.211      1.616          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:20<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217    0.00813       0.24    0.00728    0.00211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/70      12.1G      1.325      2.215      1.665          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217    0.00108      0.157   0.000771   0.000268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/70      12.1G      1.282      2.114      1.609          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.237      0.332        0.2     0.0951\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/70      12.1G      1.219      1.993      1.557          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.296      0.387      0.284       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/70      12.1G      1.122      1.917      1.521          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.556      0.223      0.207      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/70      12.1G      1.106      1.857      1.479          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.31      0.378      0.282      0.174\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/70      12.1G      1.034      1.821      1.439          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.525      0.508       0.49      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/70      12.1G     0.9712      1.691      1.387          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.494      0.481      0.489      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/70      12.1G     0.9875      1.681      1.406          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.567      0.549      0.549      0.372\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/70      12.1G     0.9519      1.625      1.378          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.574       0.54      0.546      0.361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/70      12.1G     0.8767      1.524      1.319          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.509      0.582      0.566      0.405\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/70      12.1G     0.8583      1.502      1.298          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.636      0.611       0.67      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/70      12.1G     0.8349      1.482        1.3          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.749      0.532      0.678        0.5\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/70      12.1G     0.8399      1.465      1.306          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.739      0.641       0.72      0.552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/70      12.1G      0.818      1.374      1.277          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.681       0.57      0.651      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/70      12.1G     0.7762      1.353       1.26          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.727      0.712      0.763      0.594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/70      12.1G     0.7831      1.376      1.252          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.721      0.703      0.756      0.599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/70      12.1G     0.7228      1.315       1.22          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.751      0.682      0.774      0.608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/70      12.1G     0.7338      1.315       1.22          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.745      0.719      0.777      0.609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/70      12.1G     0.7412      1.282      1.227          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.742      0.649      0.738       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/70      12.1G     0.7329      1.228      1.221          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.784      0.711      0.793      0.622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/70      12.1G     0.7026      1.177      1.197          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.801      0.754      0.825      0.675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/70      12.1G     0.6689      1.152      1.188          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.811      0.762      0.839      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/70      12.1G     0.6921      1.154      1.193          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.724      0.669      0.775      0.641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/70      12.1G     0.6819      1.176      1.183          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.794      0.755      0.847      0.718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/70      12.1G     0.6411      1.112      1.163          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.803      0.703      0.817      0.665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/70      12.1G     0.6481      1.135      1.174          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.851      0.754      0.862      0.723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/70      12.1G     0.6419      1.063      1.155          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.854      0.776      0.881      0.743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/70      12.1G     0.6248      1.087      1.146          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.82      0.794      0.879      0.748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/70      12.1G     0.6125      1.028      1.124          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.815      0.797      0.875      0.747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/70      12.1G     0.6262      1.025      1.152          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.813      0.816      0.878      0.755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/70      12.1G     0.6215     0.9772      1.145          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.842      0.793      0.875      0.764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/70      12.1G     0.5796     0.9597      1.117          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.872      0.813      0.903      0.778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/70      12.1G     0.5931     0.9841      1.124          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.82      0.838      0.899      0.783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/70      12.1G     0.5678     0.9819      1.099          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.862      0.824      0.903      0.792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/70      12.1G     0.5697     0.9338      1.107          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.856      0.857      0.926      0.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/70      12.1G      0.555      0.898      1.093          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.835      0.875       0.92      0.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/70      12.1G     0.5578     0.9266      1.104          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.882      0.865      0.938      0.843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/70      12.1G     0.5079     0.8514      1.066          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.886      0.864      0.938       0.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/70      12.1G     0.5332     0.8661      1.089          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.887      0.855      0.934      0.841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/70      12.1G       0.52     0.8313      1.071          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.902      0.878      0.945      0.849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/70      12.1G     0.5152     0.8482      1.069          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.908      0.891      0.951       0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/70      12.1G     0.5085     0.8197      1.064          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.908      0.888       0.95       0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/70      12.1G     0.5319     0.8821      1.092          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.883      0.832      0.936      0.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/70      12.1G     0.5015     0.7844      1.051          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.893      0.898      0.953      0.876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/70      12.1G     0.4902     0.7665      1.055          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.925      0.906      0.964       0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/70      12.1G     0.4967     0.7673      1.047          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.947        0.9      0.971      0.902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/70      12.1G      0.453     0.7331      1.026          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.934      0.921      0.971      0.905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/70      12.1G     0.4776     0.8269      1.042          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.92      0.925      0.968      0.902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      51/70      12.1G     0.4694     0.7256      1.044          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.927      0.898      0.966      0.906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      52/70      12.1G     0.4611      0.705      1.038          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.935       0.91      0.972       0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      53/70      12.1G     0.4494     0.6948      1.031          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.945      0.931      0.977      0.925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      54/70      12.1G     0.4611      0.691      1.043          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.952      0.932      0.976       0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      55/70      12.1G     0.4575     0.6838      1.025          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.958      0.942      0.982      0.934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      56/70      12.1G      0.442     0.6672      1.038          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.971      0.929      0.982      0.934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      57/70      12.1G     0.4347     0.6539      1.009          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.962      0.941      0.983      0.935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      58/70      12.1G     0.4078     0.6274     0.9984          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.961      0.933      0.984      0.938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      59/70      12.1G     0.4322     0.6415      1.016          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.968      0.949      0.987      0.942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      60/70      12.1G     0.3923     0.5887     0.9883          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.95      0.958      0.984      0.941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      61/70      12.1G     0.3395     0.5501      1.015          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:51<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.961      0.961      0.985      0.946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      62/70      12.1G     0.3053     0.4334     0.9521          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.966      0.957      0.984      0.946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      63/70      12.1G     0.3074     0.4594     0.9664          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:50<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.954      0.971      0.985       0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      64/70      12.1G     0.3038     0.3868     0.9526          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.973      0.973      0.985       0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      65/70      12.1G     0.2864     0.3497     0.9375          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217       0.97      0.973      0.988      0.959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      66/70      12.1G     0.2764       0.35     0.9319          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.975      0.978       0.99      0.961\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      67/70      12.1G     0.2685     0.3343     0.9274          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.977      0.979      0.992      0.964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      68/70      12.1G      0.265     0.3106     0.9221          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.983      0.978      0.991      0.965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      69/70      12.1G     0.2559     0.3027      0.905          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.982      0.981      0.992      0.966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      70/70      12.1G     0.2643     0.3421     0.9295          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:49<00:00,  1.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.982      0.983      0.992      0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "70 epochs completed in 1.411 hours.\n",
            "Optimizer stripped from runs/detect/Avito_Hack_v4/weights/last.pt, 87.6MB\n",
            "Optimizer stripped from runs/detect/Avito_Hack_v4/weights/best.pt, 87.6MB\n",
            "\n",
            "Validating runs/detect/Avito_Hack_v4/weights/best.pt...\n",
            "Ultralytics 8.3.88 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 112 layers, 43,609,692 parameters, 0 gradients, 164.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:21<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        993       1217      0.982      0.983      0.992      0.967\n",
            "                  Bags        237        251      0.996      0.995      0.995      0.973\n",
            "                Chairs        228        340       0.97      0.957      0.989      0.956\n",
            "    Clothes_baby_girls        112        144       0.97      0.993      0.989      0.954\n",
            "                Tables        466        482      0.992      0.988      0.995      0.985\n",
            "Speed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/Avito_Hack_v4\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/runs/detect/Avito_Hack_v4/weights/best.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "k8F_iASvmTh-",
        "outputId": "4b1f400d-a7f4-4ef8-8890-cd8eaf18b3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19700fb7-df8f-4cf5-865f-fc61f4fc5fdd\", \"best.pt\", 87621363)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMAuIQhEoWpP",
        "outputId": "47592889-e87f-4ac3-bd22-1977cbc56a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "\n",
        "file_id = \"1BS5KRmOe6oQc7GZijdPsCO7hWfBTWHJN\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"train.zip\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "wsnJFz2BpNc4",
        "outputId": "f4b1efa1-3a99-45b4-bb31-27e67c9741bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BS5KRmOe6oQc7GZijdPsCO7hWfBTWHJN\n",
            "From (redirected): https://drive.google.com/uc?id=1BS5KRmOe6oQc7GZijdPsCO7hWfBTWHJN&confirm=t&uuid=0f521f5c-e277-494b-90f2-62990b25f6d8\n",
            "To: /content/train.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.61G/8.61G [02:55<00:00, 49.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"train.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ],
      "metadata": {
        "id": "LnwI569YqYSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
        "model = YOLO('/content/runs/detect/Avito_Hack_v4/weights/best.pt')\n",
        "\n",
        "# ÐŸÐ°Ð¿ÐºÐ° Ñ Ð½ÐµÑ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸\n",
        "unlabeled_path = '/content/train_data/'\n",
        "# ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ñ„Ð¾Ñ‚Ð¾ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒÑŽ\n",
        "high_confidence_base_path = '/content/high_confidence_data/'\n",
        "os.makedirs(high_confidence_base_path, exist_ok=True)\n",
        "\n",
        "# ÐŸÐ¾Ñ€Ð¾Ð³ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸\n",
        "confidence_threshold = 0.8\n",
        "\n",
        "# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ„Ð°Ð¹Ð»Ñ‹ Ð² Ð¿Ð°Ð¿ÐºÐµ Ñ Ð½ÐµÑ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸\n",
        "if not os.listdir(unlabeled_path):\n",
        "    raise ValueError(\"ÐŸÐ°Ð¿ÐºÐ° Ñ Ð½ÐµÑ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¿ÑƒÑÑ‚Ð°!\")\n",
        "\n",
        "for iteration in range(5):  # 5 ÐºÑ€ÑƒÐ³Ð¾Ð² ÑÐ°Ð¼Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
        "    print(f\"Iteration {iteration + 1}\")\n",
        "\n",
        "    # Ð¨Ð°Ð³ 1: ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð½Ð° Ð½ÐµÑ€Ð°Ð·Ð¼ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
        "    current_high_confidence_path = os.path.join(high_confidence_base_path, f'iter{iteration}')\n",
        "    os.makedirs(current_high_confidence_path, exist_ok=True)\n",
        "\n",
        "    for image_name in os.listdir(unlabeled_path):\n",
        "        image_path = os.path.join(unlabeled_path, image_name)\n",
        "        results = model.predict(source=image_path, save=False, conf=confidence_threshold)\n",
        "\n",
        "        if len(results[0].boxes) > 0:\n",
        "            # ÐšÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ð¾Ñ‚Ð¾ Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ñ„Ð°Ð¹Ð» Ñ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¾Ð¹\n",
        "            shutil.copy(image_path, os.path.join(current_high_confidence_path, image_name))\n",
        "            with open(os.path.join(current_high_confidence_path, image_name.replace('.jpg', '.txt')), 'w') as f:\n",
        "                for box in results[0].boxes:\n",
        "                    class_id = int(box.cls)\n",
        "                    x_center, y_center, width, height = box.xywhn[0].tolist()\n",
        "                    f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "    # Ð¨Ð°Ð³ 2: ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n",
        "    for image_name in os.listdir(current_high_confidence_path):\n",
        "        if image_name.endswith('.jpg'):\n",
        "            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð»Ð¸ Ñ„Ð°Ð¹Ð» Ð² Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð¿Ð°Ð¿ÐºÐµ\n",
        "            dest_image_path = os.path.join('/content/dataset_train/one/images/', image_name)\n",
        "            dest_label_path = os.path.join('/content/dataset_train/one/labels/', image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "            if not os.path.exists(dest_image_path):\n",
        "                shutil.move(os.path.join(current_high_confidence_path, image_name), dest_image_path)\n",
        "            if not os.path.exists(dest_label_path):\n",
        "                shutil.move(os.path.join(current_high_confidence_path, image_name.replace('.jpg', '.txt')), dest_label_path)\n",
        "\n",
        "    # Ð¨Ð°Ð³ 3: Ð”Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "    results = model.train(\n",
        "        data='/content/dataset_train/one/dataset.yaml',\n",
        "        epochs=50,\n",
        "        batch=16,\n",
        "        imgsz=640,\n",
        "        split=0.8,\n",
        "        name=f'Avito_Hack_v{5 + iteration}',\n",
        "        patience=10\n",
        "    )\n",
        "\n",
        "    # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ñ€Ð¾Ð³ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸\n",
        "    confidence_threshold += 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2UIhKO6rxlT",
        "outputId": "4010deb7-ace9-43eb-809b-f6723f5dade8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mÐ’Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð±Ñ‹Ð»Ð¸ Ð¾Ð±Ñ€ÐµÐ·Ð°Ð½Ñ‹ Ð´Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… ÑÑ‚Ñ€Ð¾Ðº (5000).\u001b[0m\n",
            "Speed: 12.1ms preprocess, 27.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36475099872.jpg: 640x480 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 4.1ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465256658.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 3.3ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486005952.jpg: 640x288 1 Bags, 23.2ms\n",
            "Speed: 2.5ms preprocess, 23.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/37598473419.jpg: 480x640 1 Bags, 24.6ms\n",
            "Speed: 2.6ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29622904500.jpg: 640x480 1 Chairs, 25.5ms\n",
            "Speed: 4.1ms preprocess, 25.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486739125.jpg: 640x576 (no detections), 32.5ms\n",
            "Speed: 4.4ms preprocess, 32.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36464066526.jpg: 640x384 1 Bags, 21.8ms\n",
            "Speed: 3.6ms preprocess, 21.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462572073.jpg: 640x512 1 Bags, 25.1ms\n",
            "Speed: 4.5ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37589776147.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.6ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486270334.jpg: 512x640 1 Bags, 26.0ms\n",
            "Speed: 4.4ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37594207817.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 4.2ms preprocess, 25.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469719130.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467280767.jpg: 640x640 (no detections), 33.3ms\n",
            "Speed: 3.6ms preprocess, 33.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36476114575.jpg: 640x480 1 Clothes_baby_girls, 26.1ms\n",
            "Speed: 4.4ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475861637.jpg: 640x640 1 Bags, 36.1ms\n",
            "Speed: 4.8ms preprocess, 36.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460228193.jpg: 640x640 1 Bags, 34.6ms\n",
            "Speed: 5.1ms preprocess, 34.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36468653463.jpg: 512x640 1 Bags, 26.4ms\n",
            "Speed: 2.7ms preprocess, 26.4ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29600421238.jpg: 384x640 1 Tables, 23.0ms\n",
            "Speed: 3.3ms preprocess, 23.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29651814853.jpg: 480x640 1 Chairs, 1 Tables, 25.1ms\n",
            "Speed: 2.5ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36477358078.jpg: 640x480 1 Bags, 25.4ms\n",
            "Speed: 4.4ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471835423.jpg: 480x640 1 Bags, 26.5ms\n",
            "Speed: 4.6ms preprocess, 26.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461482842.jpg: 640x448 (no detections), 25.0ms\n",
            "Speed: 4.1ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36456084701.jpg: 640x448 1 Bags, 23.6ms\n",
            "Speed: 3.9ms preprocess, 23.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36456677508.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462818754.jpg: 640x640 (no detections), 33.3ms\n",
            "Speed: 4.7ms preprocess, 33.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29625842525.jpg: 448x640 1 Tables, 30.3ms\n",
            "Speed: 3.6ms preprocess, 30.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466441521.jpg: 480x640 (no detections), 25.2ms\n",
            "Speed: 3.8ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462764074.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598886458.jpg: 480x640 (no detections), 26.6ms\n",
            "Speed: 3.1ms preprocess, 26.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457033014.jpg: 480x640 1 Tables, 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474311218.jpg: 640x480 1 Bags, 25.5ms\n",
            "Speed: 4.0ms preprocess, 25.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36434173557.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589711399.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 2.4ms preprocess, 23.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467928816.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 3.0ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29641278416.jpg: 576x640 1 Tables, 32.8ms\n",
            "Speed: 4.5ms preprocess, 32.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37594170008.jpg: 640x480 (no detections), 27.1ms\n",
            "Speed: 2.6ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472108241.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.7ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588646824.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.1ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458165033.jpg: 640x320 1 Clothes_baby_girls, 20.4ms\n",
            "Speed: 2.7ms preprocess, 20.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36459615342.jpg: 640x480 1 Clothes_baby_girls, 24.2ms\n",
            "Speed: 3.9ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486465382.jpg: 480x640 1 Clothes_baby_girls, 25.3ms\n",
            "Speed: 2.4ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457351574.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.0ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468299171.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 3.0ms preprocess, 23.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486163157.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 4.4ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464958529.jpg: 640x640 1 Bags, 32.7ms\n",
            "Speed: 3.4ms preprocess, 32.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37599192175.jpg: 640x480 (no detections), 26.0ms\n",
            "Speed: 4.1ms preprocess, 26.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461232771.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 2.9ms preprocess, 23.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616938487.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 4.3ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589253820.jpg: 640x448 1 Bags, 25.5ms\n",
            "Speed: 3.9ms preprocess, 25.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/29610111952.jpg: 640x480 1 Tables, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29608511007.jpg: 640x640 1 Tables, 37.0ms\n",
            "Speed: 4.2ms preprocess, 37.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464536100.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 2.5ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464866953.jpg: 640x384 (no detections), 21.2ms\n",
            "Speed: 3.3ms preprocess, 21.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36458348627.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.5ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476256370.jpg: 576x640 1 Bags, 32.8ms\n",
            "Speed: 5.0ms preprocess, 32.8ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593652092.jpg: 640x512 1 Bags, 25.0ms\n",
            "Speed: 3.2ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36468729487.jpg: 480x640 1 Clothes_baby_girls, 26.1ms\n",
            "Speed: 2.7ms preprocess, 26.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36471401736.jpg: 640x320 1 Bags, 19.9ms\n",
            "Speed: 2.5ms preprocess, 19.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36458857186.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.8ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482382126.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 4.2ms preprocess, 23.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462904357.jpg: 640x576 1 Bags, 33.6ms\n",
            "Speed: 5.2ms preprocess, 33.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/37593782233.jpg: 640x480 (no detections), 25.9ms\n",
            "Speed: 4.1ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651069713.jpg: 640x320 (no detections), 20.8ms\n",
            "Speed: 3.3ms preprocess, 20.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36464295982.jpg: 480x640 (no detections), 27.1ms\n",
            "Speed: 4.3ms preprocess, 27.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466867447.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477052761.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 2.6ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597855453.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.3ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588391191.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 4.1ms preprocess, 23.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588350383.jpg: 480x640 1 Clothes_baby_girls, 26.5ms\n",
            "Speed: 2.7ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29611369776.jpg: 480x640 1 Tables, 24.9ms\n",
            "Speed: 4.3ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469451166.jpg: 640x640 1 Bags, 32.5ms\n",
            "Speed: 4.7ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29649301119.jpg: 640x288 1 Tables, 20.7ms\n",
            "Speed: 2.8ms preprocess, 20.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/29613091798.jpg: 640x288 1 Chairs, 18.7ms\n",
            "Speed: 2.1ms preprocess, 18.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36456975894.jpg: 480x640 (no detections), 25.6ms\n",
            "Speed: 2.6ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29651489946.jpg: 480x640 1 Tables, 25.4ms\n",
            "Speed: 2.6ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588592247.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 2.5ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616234909.jpg: 640x480 1 Chairs, 24.6ms\n",
            "Speed: 2.5ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484337345.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 4.2ms preprocess, 23.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588531166.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.2ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480337241.jpg: 480x640 1 Bags, 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29609839860.jpg: 640x480 1 Tables, 25.8ms\n",
            "Speed: 3.0ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464852190.jpg: 640x512 1 Clothes_baby_girls, 25.8ms\n",
            "Speed: 4.7ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36463019750.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 2.6ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590391445.jpg: 576x640 1 Clothes_baby_girls, 31.9ms\n",
            "Speed: 3.2ms preprocess, 31.9ms inference, 2.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469728522.jpg: 640x480 1 Clothes_baby_girls, 23.7ms\n",
            "Speed: 4.0ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29634553389.jpg: 640x512 1 Chairs, 25.5ms\n",
            "Speed: 3.8ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36461440319.jpg: 640x512 1 Bags, 23.1ms\n",
            "Speed: 4.4ms preprocess, 23.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36481569617.jpg: 640x288 1 Bags, 20.4ms\n",
            "Speed: 3.9ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/37590931498.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 3.1ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598044788.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.3ms preprocess, 23.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588832743.jpg: 512x640 1 Bags, 26.5ms\n",
            "Speed: 4.2ms preprocess, 26.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29648784002.jpg: 480x640 2 Chairss, 1 Tables, 29.2ms\n",
            "Speed: 3.4ms preprocess, 29.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458268919.jpg: 640x480 1 Clothes_baby_girls, 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620692688.jpg: 448x640 2 Chairss, 24.5ms\n",
            "Speed: 2.3ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36471586306.jpg: 640x640 (no detections), 32.4ms\n",
            "Speed: 3.5ms preprocess, 32.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587495321.jpg: 640x576 1 Bags, 33.5ms\n",
            "Speed: 3.2ms preprocess, 33.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/29633162370.jpg: 640x480 1 Tables, 25.7ms\n",
            "Speed: 3.0ms preprocess, 25.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460274631.jpg: 640x480 1 Clothes_baby_girls, 23.7ms\n",
            "Speed: 4.2ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472177021.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.6ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463754218.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.1ms preprocess, 24.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485289592.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460257472.jpg: 640x288 1 Bags, 19.6ms\n",
            "Speed: 2.5ms preprocess, 19.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/29649986904.jpg: 640x480 (no detections), 24.6ms\n",
            "Speed: 4.3ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29648218860.jpg: 640x384 (no detections), 20.7ms\n",
            "Speed: 3.2ms preprocess, 20.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36459689347.jpg: 640x416 1 Tables, 23.1ms\n",
            "Speed: 5.1ms preprocess, 23.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/29641055665.jpg: 384x640 1 Tables, 20.8ms\n",
            "Speed: 2.2ms preprocess, 20.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598445053.jpg: 544x640 1 Clothes_baby_girls, 31.5ms\n",
            "Speed: 2.9ms preprocess, 31.5ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463724504.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 4.2ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595064474.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 5.4ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594090991.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478481511.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587486364.jpg: 512x640 1 Bags, 27.1ms\n",
            "Speed: 2.6ms preprocess, 27.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588682514.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 3.1ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592442369.jpg: 640x480 1 Bags, 26.4ms\n",
            "Speed: 3.4ms preprocess, 26.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651007396.jpg: 640x288 1 Tables, 21.1ms\n",
            "Speed: 2.7ms preprocess, 21.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/29622450046.jpg: 640x512 2 Chairss, 26.2ms\n",
            "Speed: 3.9ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29619809592.jpg: 640x480 1 Tables, 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29622985102.jpg: 640x480 1 Tables, 24.7ms\n",
            "Speed: 2.4ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29612633263.jpg: 640x480 1 Tables, 23.3ms\n",
            "Speed: 3.0ms preprocess, 23.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482774025.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.0ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459776004.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 3.4ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460802214.jpg: 640x640 1 Bags, 33.4ms\n",
            "Speed: 3.3ms preprocess, 33.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29634278599.jpg: 640x320 1 Tables, 20.9ms\n",
            "Speed: 2.8ms preprocess, 20.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36456365672.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 2.6ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461964471.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 2.4ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479670738.jpg: 640x384 2 Bagss, 22.1ms\n",
            "Speed: 3.3ms preprocess, 22.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37589104504.jpg: 640x512 (no detections), 26.7ms\n",
            "Speed: 3.8ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37596584687.jpg: 640x480 1 Bags, 26.2ms\n",
            "Speed: 3.8ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471072763.jpg: 480x640 (no detections), 25.9ms\n",
            "Speed: 3.0ms preprocess, 25.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29640892095.jpg: 640x512 1 Tables, 25.5ms\n",
            "Speed: 4.3ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36481015909.jpg: 640x480 (no detections), 25.7ms\n",
            "Speed: 4.1ms preprocess, 25.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486814695.jpg: 480x640 1 Bags, 25.9ms\n",
            "Speed: 2.6ms preprocess, 25.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455435278.jpg: 640x480 1 Bags, 1 Tables, 26.5ms\n",
            "Speed: 3.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464548418.jpg: 640x512 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37589635333.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 2.8ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598533230.jpg: 384x640 1 Bags, 20.4ms\n",
            "Speed: 2.9ms preprocess, 20.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588288719.jpg: 640x480 1 Chairs, 24.1ms\n",
            "Speed: 3.9ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471004050.jpg: 480x640 (no detections), 25.7ms\n",
            "Speed: 2.4ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590284462.jpg: 640x608 2 Clothes_baby_girlss, 32.0ms\n",
            "Speed: 4.1ms preprocess, 32.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/37596843518.jpg: 640x640 1 Clothes_baby_girls, 34.7ms\n",
            "Speed: 5.5ms preprocess, 34.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588626700.jpg: 480x640 (no detections), 25.9ms\n",
            "Speed: 2.7ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36170958067.jpg: 640x480 1 Clothes_baby_girls, 25.0ms\n",
            "Speed: 3.4ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456211652.jpg: 640x320 1 Bags, 20.3ms\n",
            "Speed: 2.8ms preprocess, 20.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36460399071.jpg: 640x384 1 Bags, 20.5ms\n",
            "Speed: 3.3ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36467315340.jpg: 640x512 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 4.3ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36463391166.jpg: 640x640 (no detections), 32.8ms\n",
            "Speed: 3.4ms preprocess, 32.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464848448.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 4.3ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476587606.jpg: 640x448 1 Bags, 26.3ms\n",
            "Speed: 4.3ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36464206746.jpg: 640x288 1 Bags, 19.4ms\n",
            "Speed: 2.3ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/29628231309.jpg: 640x480 1 Chairs, 23.5ms\n",
            "Speed: 4.0ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595328288.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 2.9ms preprocess, 25.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457769177.jpg: 480x640 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 2.5ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590844529.jpg: 480x640 1 Bags, 25.1ms\n",
            "Speed: 2.9ms preprocess, 25.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463100336.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.0ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29625376059.jpg: 640x480 1 Chairs, 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468731269.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483633499.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461603637.jpg: 480x640 1 Bags, 26.0ms\n",
            "Speed: 4.2ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36468464458.jpg: 640x640 1 Clothes_baby_girls, 33.4ms\n",
            "Speed: 3.8ms preprocess, 33.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463425673.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.4ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36111941487.jpg: 640x544 (no detections), 32.4ms\n",
            "Speed: 4.5ms preprocess, 32.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37596763228.jpg: 640x384 (no detections), 22.1ms\n",
            "Speed: 3.3ms preprocess, 22.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36456379262.jpg: 640x480 1 Clothes_baby_girls, 25.1ms\n",
            "Speed: 4.0ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483697808.jpg: 640x480 1 Bags, 22.7ms\n",
            "Speed: 3.9ms preprocess, 22.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597490101.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 3.9ms preprocess, 23.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465839732.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 3.9ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469062106.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.0ms preprocess, 25.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29625975833.jpg: 480x640 1 Tables, 27.1ms\n",
            "Speed: 6.9ms preprocess, 27.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466318635.jpg: 640x480 1 Bags, 28.7ms\n",
            "Speed: 4.0ms preprocess, 28.7ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483881800.jpg: 640x384 1 Clothes_baby_girls, 22.4ms\n",
            "Speed: 3.3ms preprocess, 22.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37593866959.jpg: 640x480 (no detections), 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457183339.jpg: 640x576 1 Bags, 1 Clothes_baby_girls, 32.9ms\n",
            "Speed: 5.0ms preprocess, 32.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/37595217559.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 9.2ms preprocess, 24.5ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486180099.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 4.0ms preprocess, 23.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29641922443.jpg: 640x640 (no detections), 35.3ms\n",
            "Speed: 4.9ms preprocess, 35.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36467278968.jpg: 640x512 1 Bags, 25.5ms\n",
            "Speed: 4.4ms preprocess, 25.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36455783514.jpg: 640x384 (no detections), 23.5ms\n",
            "Speed: 3.1ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462049305.jpg: 480x640 1 Bags, 25.4ms\n",
            "Speed: 4.1ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29648544450.jpg: 640x480 1 Chairs, 24.2ms\n",
            "Speed: 4.0ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461057184.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467461758.jpg: 480x640 1 Bags, 29.1ms\n",
            "Speed: 4.4ms preprocess, 29.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36473434521.jpg: 640x320 2 Clothes_baby_girlss, 19.9ms\n",
            "Speed: 2.8ms preprocess, 19.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36483567145.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.3ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588117065.jpg: 640x416 1 Clothes_baby_girls, 23.7ms\n",
            "Speed: 5.3ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36459604079.jpg: 640x480 1 Bags, 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587785141.jpg: 640x640 1 Bags, 34.4ms\n",
            "Speed: 5.4ms preprocess, 34.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466948046.jpg: 480x640 1 Clothes_baby_girls, 28.4ms\n",
            "Speed: 9.6ms preprocess, 28.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463634354.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 6.0ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591638725.jpg: 640x384 1 Bags, 26.1ms\n",
            "Speed: 6.6ms preprocess, 26.1ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36463860075.jpg: 640x480 1 Bags, 34.2ms\n",
            "Speed: 3.9ms preprocess, 34.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458895325.jpg: 640x480 1 Bags, 28.2ms\n",
            "Speed: 3.9ms preprocess, 28.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481190383.jpg: 640x512 1 Tables, 25.4ms\n",
            "Speed: 4.9ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36461890612.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 4.2ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594992451.jpg: 480x640 (no detections), 26.6ms\n",
            "Speed: 4.1ms preprocess, 26.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590680415.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.0ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591142521.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.1ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462774766.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 3.9ms preprocess, 24.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29603117646.jpg: 640x512 1 Tables, 25.5ms\n",
            "Speed: 4.1ms preprocess, 25.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36486252106.jpg: 640x480 1 Clothes_baby_girls, 34.2ms\n",
            "Speed: 3.9ms preprocess, 34.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462615571.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 5.8ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457660028.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 4.1ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590937981.jpg: 640x640 (no detections), 34.8ms\n",
            "Speed: 5.0ms preprocess, 34.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29640973766.jpg: 480x640 1 Tables, 27.5ms\n",
            "Speed: 4.2ms preprocess, 27.5ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29635890109.jpg: 320x640 1 Tables, 20.8ms\n",
            "Speed: 2.9ms preprocess, 20.8ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463264292.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 3.1ms preprocess, 25.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588686283.jpg: 640x480 (no detections), 26.1ms\n",
            "Speed: 4.0ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459911360.jpg: 640x480 (no detections), 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485294004.jpg: 640x544 3 Clothes_baby_girlss, 1 Tables, 31.7ms\n",
            "Speed: 4.6ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36484220689.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.0ms preprocess, 24.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596768980.jpg: 640x480 1 Clothes_baby_girls, 24.3ms\n",
            "Speed: 4.1ms preprocess, 24.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651829391.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 5.9ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29626113560.jpg: 480x640 1 Chairs, 26.7ms\n",
            "Speed: 2.6ms preprocess, 26.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466064530.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 6.1ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597589899.jpg: 640x640 (no detections), 32.9ms\n",
            "Speed: 5.4ms preprocess, 32.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457278752.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484758247.jpg: 640x544 1 Bags, 35.8ms\n",
            "Speed: 4.3ms preprocess, 35.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37598894893.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458268708.jpg: 640x384 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 7.2ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37591380887.jpg: 640x352 1 Bags, 20.0ms\n",
            "Speed: 2.0ms preprocess, 20.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 352)\n",
            "\n",
            "image 1/1 /content/train_data/29636075225.jpg: 640x480 1 Chairs, 24.8ms\n",
            "Speed: 3.9ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464077376.jpg: 640x480 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 3.8ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590056798.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484340366.jpg: 480x640 (no detections), 25.7ms\n",
            "Speed: 8.5ms preprocess, 25.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459485988.jpg: 640x480 (no detections), 25.5ms\n",
            "Speed: 4.8ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474990654.jpg: 640x480 1 Clothes_baby_girls, 25.9ms\n",
            "Speed: 4.9ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464929138.jpg: 640x480 1 Clothes_baby_girls, 23.8ms\n",
            "Speed: 2.7ms preprocess, 23.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463499097.jpg: 480x640 1 Bags, 36.2ms\n",
            "Speed: 4.1ms preprocess, 36.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461815864.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.5ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459284539.jpg: 576x640 1 Bags, 29.9ms\n",
            "Speed: 8.9ms preprocess, 29.9ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485717124.jpg: 640x640 1 Bags, 36.2ms\n",
            "Speed: 4.6ms preprocess, 36.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466934836.jpg: 640x640 (no detections), 30.9ms\n",
            "Speed: 5.2ms preprocess, 30.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463739333.jpg: 608x640 1 Bags, 33.0ms\n",
            "Speed: 7.2ms preprocess, 33.0ms inference, 1.8ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29610656587.jpg: 640x480 1 Tables, 25.2ms\n",
            "Speed: 7.3ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485473294.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 8.0ms preprocess, 23.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484200145.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 5.0ms preprocess, 23.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29644775200.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 4.5ms preprocess, 23.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598640249.jpg: 640x384 1 Bags, 21.7ms\n",
            "Speed: 7.9ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36483375271.jpg: 448x640 (no detections), 27.8ms\n",
            "Speed: 3.1ms preprocess, 27.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37596626447.jpg: 640x384 1 Bags, 33.9ms\n",
            "Speed: 3.3ms preprocess, 33.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36478850855.jpg: 640x640 1 Bags, 32.9ms\n",
            "Speed: 9.0ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29623584083.jpg: 640x480 1 Chairs, 25.1ms\n",
            "Speed: 6.5ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590211392.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 5.4ms preprocess, 23.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471330257.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.7ms preprocess, 24.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459129873.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 4.5ms preprocess, 23.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591067854.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 7.1ms preprocess, 26.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484214710.jpg: 384x640 1 Bags, 20.9ms\n",
            "Speed: 3.6ms preprocess, 20.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463716114.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.4ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466083130.jpg: 640x480 1 Clothes_baby_girls, 24.0ms\n",
            "Speed: 5.6ms preprocess, 24.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462092278.jpg: 576x640 1 Bags, 1 Clothes_baby_girls, 33.0ms\n",
            "Speed: 8.9ms preprocess, 33.0ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458455029.jpg: 640x640 2 Clothes_baby_girlss, 33.0ms\n",
            "Speed: 8.3ms preprocess, 33.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469648220.jpg: 640x576 1 Bags, 33.6ms\n",
            "Speed: 5.4ms preprocess, 33.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36470705387.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.4ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29631760770.jpg: 480x640 2 Chairss, 26.7ms\n",
            "Speed: 4.2ms preprocess, 26.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36482279947.jpg: 640x480 1 Bags, 26.0ms\n",
            "Speed: 6.7ms preprocess, 26.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462295386.jpg: 480x640 (no detections), 26.7ms\n",
            "Speed: 4.3ms preprocess, 26.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466527270.jpg: 480x640 1 Bags, 30.8ms\n",
            "Speed: 4.7ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587996765.jpg: 640x384 1 Clothes_baby_girls, 29.4ms\n",
            "Speed: 3.4ms preprocess, 29.4ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36481626269.jpg: 640x480 1 Bags, 29.1ms\n",
            "Speed: 5.4ms preprocess, 29.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464633011.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 7.1ms preprocess, 23.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29634839320.jpg: 640x480 1 Tables, 25.2ms\n",
            "Speed: 4.5ms preprocess, 25.2ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29629719904.jpg: 640x512 1 Chairs, 28.5ms\n",
            "Speed: 6.3ms preprocess, 28.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29642389781.jpg: 640x384 1 Chairs, 21.9ms\n",
            "Speed: 3.3ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37596208378.jpg: 480x640 (no detections), 25.8ms\n",
            "Speed: 7.1ms preprocess, 25.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588906010.jpg: 480x640 1 Bags, 26.1ms\n",
            "Speed: 8.4ms preprocess, 26.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29650435470.jpg: 640x288 1 Tables, 21.5ms\n",
            "Speed: 5.5ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/29646136345.jpg: 640x640 (no detections), 32.9ms\n",
            "Speed: 5.8ms preprocess, 32.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29605038427.jpg: 480x640 2 Chairss, 27.9ms\n",
            "Speed: 4.6ms preprocess, 27.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465857833.jpg: 640x480 1 Clothes_baby_girls, 27.3ms\n",
            "Speed: 4.1ms preprocess, 27.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29966221355.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.5ms preprocess, 23.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481253020.jpg: 640x640 (no detections), 36.4ms\n",
            "Speed: 5.3ms preprocess, 36.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36468133375.jpg: 608x640 1 Bags, 34.0ms\n",
            "Speed: 5.6ms preprocess, 34.0ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464288862.jpg: 640x480 (no detections), 25.5ms\n",
            "Speed: 4.5ms preprocess, 25.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588037570.jpg: 480x640 1 Bags, 26.2ms\n",
            "Speed: 3.1ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465771370.jpg: 640x640 1 Bags, 43.2ms\n",
            "Speed: 7.5ms preprocess, 43.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29615671572.jpg: 640x480 1 Tables, 27.6ms\n",
            "Speed: 4.0ms preprocess, 27.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486180217.jpg: 640x640 (no detections), 33.6ms\n",
            "Speed: 7.0ms preprocess, 33.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597760923.jpg: 640x480 1 Bags, 26.3ms\n",
            "Speed: 3.4ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465971971.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 4.5ms preprocess, 23.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486652836.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 9.6ms preprocess, 23.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456223318.jpg: 640x480 1 Clothes_baby_girls, 27.7ms\n",
            "Speed: 2.9ms preprocess, 27.7ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484313777.jpg: 640x480 1 Bags, 22.8ms\n",
            "Speed: 4.0ms preprocess, 22.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29650303355.jpg: 640x384 1 Tables, 24.9ms\n",
            "Speed: 6.7ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29629740964.jpg: 640x480 1 Tables, 26.4ms\n",
            "Speed: 4.5ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595643598.jpg: 640x320 (no detections), 21.5ms\n",
            "Speed: 2.6ms preprocess, 21.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36483602845.jpg: 480x640 (no detections), 26.3ms\n",
            "Speed: 4.2ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459126263.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 3.4ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472103691.jpg: 608x640 (no detections), 34.3ms\n",
            "Speed: 4.3ms preprocess, 34.3ms inference, 0.5ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463695467.jpg: 640x480 1 Clothes_baby_girls, 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29629685817.jpg: 640x480 1 Tables, 24.2ms\n",
            "Speed: 4.1ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463614028.jpg: 480x640 1 Bags, 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589539051.jpg: 640x480 1 Bags, 25.4ms\n",
            "Speed: 3.0ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458419591.jpg: 640x640 1 Bags, 33.5ms\n",
            "Speed: 4.2ms preprocess, 33.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461535959.jpg: 640x384 (no detections), 21.8ms\n",
            "Speed: 3.5ms preprocess, 21.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29642539348.jpg: 480x640 1 Chairs, 26.8ms\n",
            "Speed: 2.9ms preprocess, 26.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37591011450.jpg: 640x384 1 Bags, 21.8ms\n",
            "Speed: 3.7ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36478366131.jpg: 480x640 (no detections), 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593492553.jpg: 640x544 (no detections), 32.5ms\n",
            "Speed: 3.5ms preprocess, 32.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36460452506.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 3.4ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480218440.jpg: 480x640 1 Bags, 26.1ms\n",
            "Speed: 2.4ms preprocess, 26.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458285793.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462499492.jpg: 448x640 1 Bags, 27.0ms\n",
            "Speed: 3.6ms preprocess, 27.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589302541.jpg: 640x480 (no detections), 24.8ms\n",
            "Speed: 2.5ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29619411817.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 4.1ms preprocess, 24.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483198536.jpg: 480x640 2 Clothes_baby_girlss, 26.4ms\n",
            "Speed: 3.6ms preprocess, 26.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36476269312.jpg: 640x480 1 Clothes_baby_girls, 25.2ms\n",
            "Speed: 3.9ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29627053295.jpg: 640x480 1 Tables, 24.5ms\n",
            "Speed: 4.3ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463669095.jpg: 640x640 (no detections), 32.3ms\n",
            "Speed: 5.5ms preprocess, 32.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36471036526.jpg: 480x640 1 Clothes_baby_girls, 26.7ms\n",
            "Speed: 2.6ms preprocess, 26.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36483584518.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.4ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467230235.jpg: 448x640 2 Clothes_baby_girlss, 25.8ms\n",
            "Speed: 2.6ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464764834.jpg: 480x640 (no detections), 25.4ms\n",
            "Speed: 2.6ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589858513.jpg: 640x480 1 Clothes_baby_girls, 25.5ms\n",
            "Speed: 4.1ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465181665.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 3.9ms preprocess, 25.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598962810.jpg: 640x480 1 Clothes_baby_girls, 24.9ms\n",
            "Speed: 5.2ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477324437.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 3.1ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464142373.jpg: 640x544 (no detections), 34.9ms\n",
            "Speed: 3.8ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36468741888.jpg: 640x512 1 Bags, 25.3ms\n",
            "Speed: 4.3ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36465392192.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.3ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36291694787.jpg: 640x384 1 Clothes_baby_girls, 21.4ms\n",
            "Speed: 3.5ms preprocess, 21.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36471355691.jpg: 640x320 1 Bags, 19.3ms\n",
            "Speed: 2.5ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29604144520.jpg: 640x576 1 Chairs, 30.4ms\n",
            "Speed: 4.9ms preprocess, 30.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/29626850664.jpg: 640x480 1 Chairs, 25.3ms\n",
            "Speed: 3.2ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607618430.jpg: 640x480 1 Chairs, 24.4ms\n",
            "Speed: 3.4ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461962949.jpg: 640x608 (no detections), 32.0ms\n",
            "Speed: 3.1ms preprocess, 32.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36467233280.jpg: 448x640 (no detections), 26.9ms\n",
            "Speed: 2.4ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464736381.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 3.6ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462708922.jpg: 480x640 1 Clothes_baby_girls, 25.8ms\n",
            "Speed: 2.5ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474122833.jpg: 640x480 (no detections), 25.9ms\n",
            "Speed: 4.1ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588637939.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 4.3ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29896525798.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.8ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588540225.jpg: 640x512 1 Clothes_baby_girls, 25.3ms\n",
            "Speed: 4.3ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37590395912.jpg: 640x480 4 Clothes_baby_girlss, 25.1ms\n",
            "Speed: 2.5ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590278009.jpg: 640x384 1 Clothes_baby_girls, 20.7ms\n",
            "Speed: 3.4ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462117677.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459218737.jpg: 640x640 2 Bagss, 4 Clothes_baby_girlss, 30.7ms\n",
            "Speed: 4.0ms preprocess, 30.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595637618.jpg: 480x640 (no detections), 26.7ms\n",
            "Speed: 2.9ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36454949133.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29628921089.jpg: 640x640 1 Tables, 33.5ms\n",
            "Speed: 4.6ms preprocess, 33.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29652463017.jpg: 640x480 1 Tables, 26.5ms\n",
            "Speed: 4.2ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482254613.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.1ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485527896.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.1ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484858435.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.2ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29649413330.jpg: 416x640 1 Tables, 27.8ms\n",
            "Speed: 3.6ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37592988865.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472545242.jpg: 640x640 1 Clothes_baby_girls, 31.9ms\n",
            "Speed: 4.9ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469015091.jpg: 640x480 1 Clothes_baby_girls, 25.2ms\n",
            "Speed: 3.7ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483734745.jpg: 480x640 1 Bags, 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37586451433.jpg: 640x320 1 Bags, 20.2ms\n",
            "Speed: 2.2ms preprocess, 20.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36472330738.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471416714.jpg: 640x640 1 Bags, 34.8ms\n",
            "Speed: 4.9ms preprocess, 34.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455443610.jpg: 640x640 (no detections), 32.4ms\n",
            "Speed: 4.4ms preprocess, 32.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459487420.jpg: 640x480 (no detections), 25.8ms\n",
            "Speed: 4.4ms preprocess, 25.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468715571.jpg: 608x640 1 Bags, 34.0ms\n",
            "Speed: 5.0ms preprocess, 34.0ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462616597.jpg: 640x480 1 Bags, 25.5ms\n",
            "Speed: 2.8ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29612640048.jpg: 640x480 1 Tables, 23.5ms\n",
            "Speed: 4.5ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598731982.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.3ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466913656.jpg: 480x640 (no detections), 27.1ms\n",
            "Speed: 2.6ms preprocess, 27.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595405793.jpg: 640x512 1 Bags, 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37598063431.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 3.8ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592365495.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 2.7ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461274811.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483022617.jpg: 480x640 (no detections), 25.8ms\n",
            "Speed: 2.6ms preprocess, 25.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37591724930.jpg: 640x480 1 Clothes_baby_girls, 24.0ms\n",
            "Speed: 3.9ms preprocess, 24.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486178165.jpg: 640x640 1 Bags, 32.8ms\n",
            "Speed: 5.4ms preprocess, 32.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486556723.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 4.4ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470338603.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.3ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36451824573.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 2.5ms preprocess, 22.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485969225.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 3.2ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29623302560.jpg: 640x384 1 Tables, 20.4ms\n",
            "Speed: 3.3ms preprocess, 20.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36483921360.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589585168.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 3.8ms preprocess, 23.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466007240.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 4.4ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460359973.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 4.4ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480153374.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 3.9ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596796361.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 2.9ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461474672.jpg: 640x288 (no detections), 20.4ms\n",
            "Speed: 2.8ms preprocess, 20.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36455857672.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 3.8ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29610449206.jpg: 640x480 1 Tables, 23.2ms\n",
            "Speed: 4.1ms preprocess, 23.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464738925.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 3.2ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472577584.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.3ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596729077.jpg: 352x640 1 Bags, 19.8ms\n",
            "Speed: 3.2ms preprocess, 19.8ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461614158.jpg: 640x512 1 Bags, 23.7ms\n",
            "Speed: 4.3ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37589690307.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 5.0ms preprocess, 23.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459364809.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462649961.jpg: 480x640 1 Clothes_baby_girls, 26.1ms\n",
            "Speed: 2.5ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36467091739.jpg: 640x544 1 Bags, 30.8ms\n",
            "Speed: 4.4ms preprocess, 30.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36476792538.jpg: 640x480 1 Bags, 25.4ms\n",
            "Speed: 3.2ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29640262218.jpg: 640x480 1 Tables, 23.4ms\n",
            "Speed: 2.8ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485061066.jpg: 480x640 1 Bags, 25.6ms\n",
            "Speed: 2.6ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486462085.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.2ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474379214.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477168331.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 3.0ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469978024.jpg: 640x480 (no detections), 24.6ms\n",
            "Speed: 4.1ms preprocess, 24.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485417617.jpg: 640x512 1 Bags, 24.2ms\n",
            "Speed: 2.6ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36460573642.jpg: 640x480 1 Clothes_baby_girls, 25.0ms\n",
            "Speed: 2.5ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463653813.jpg: 480x640 1 Clothes_baby_girls, 25.7ms\n",
            "Speed: 2.5ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485794413.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.8ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467323695.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.1ms preprocess, 23.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29619879510.jpg: 544x640 1 Tables, 30.1ms\n",
            "Speed: 2.8ms preprocess, 30.1ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456101984.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 2.9ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589000891.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.8ms preprocess, 24.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468352863.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480951213.jpg: 640x480 1 Clothes_baby_girls, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480057888.jpg: 640x480 2 Bagss, 22.8ms\n",
            "Speed: 9.0ms preprocess, 22.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459374967.jpg: 640x480 1 Bags, 23.0ms\n",
            "Speed: 2.5ms preprocess, 23.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590247990.jpg: 640x640 1 Clothes_baby_girls, 32.4ms\n",
            "Speed: 5.6ms preprocess, 32.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460605226.jpg: 640x288 1 Clothes_baby_girls, 21.0ms\n",
            "Speed: 2.8ms preprocess, 21.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/37597217840.jpg: 320x640 (no detections), 19.3ms\n",
            "Speed: 2.7ms preprocess, 19.3ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29625041269.jpg: 288x640 1 Tables, 18.2ms\n",
            "Speed: 2.0ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588982850.jpg: 640x480 1 Clothes_baby_girls, 23.9ms\n",
            "Speed: 4.0ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461307648.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.4ms preprocess, 24.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478205930.jpg: 640x512 1 Bags, 25.1ms\n",
            "Speed: 5.1ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29612721874.jpg: 640x352 1 Chairs, 20.7ms\n",
            "Speed: 3.4ms preprocess, 20.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
            "\n",
            "image 1/1 /content/train_data/29618969647.jpg: 640x480 1 Tables, 24.9ms\n",
            "Speed: 3.8ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472000922.jpg: 640x512 (no detections), 24.6ms\n",
            "Speed: 4.4ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36462547600.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 2.5ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29622392191.jpg: 480x640 1 Chairs, 1 Tables, 25.6ms\n",
            "Speed: 2.4ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458840024.jpg: 480x640 1 Tables, 23.9ms\n",
            "Speed: 4.1ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462016708.jpg: 640x384 1 Clothes_baby_girls, 22.7ms\n",
            "Speed: 3.3ms preprocess, 22.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37595462988.jpg: 640x640 (no detections), 33.4ms\n",
            "Speed: 3.4ms preprocess, 33.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590454734.jpg: 640x512 1 Bags, 26.4ms\n",
            "Speed: 3.3ms preprocess, 26.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36464005860.jpg: 640x384 1 Bags, 21.6ms\n",
            "Speed: 3.4ms preprocess, 21.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36466371821.jpg: 480x640 (no detections), 27.5ms\n",
            "Speed: 2.6ms preprocess, 27.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29649709226.jpg: 384x640 1 Tables, 20.3ms\n",
            "Speed: 1.9ms preprocess, 20.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464804752.jpg: 640x480 2 Clothes_baby_girlss, 24.8ms\n",
            "Speed: 2.6ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475463358.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 3.8ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486024498.jpg: 640x480 2 Clothes_baby_girlss, 24.6ms\n",
            "Speed: 3.3ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467626723.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.1ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469541790.jpg: 640x480 (no detections), 24.3ms\n",
            "Speed: 3.9ms preprocess, 24.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484864218.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 6.0ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460176957.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.1ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480094199.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 3.2ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589815789.jpg: 480x640 1 Tables, 26.2ms\n",
            "Speed: 4.2ms preprocess, 26.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36470026625.jpg: 640x480 1 Clothes_baby_girls, 26.0ms\n",
            "Speed: 4.2ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481259752.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594286292.jpg: 640x640 1 Clothes_baby_girls, 31.8ms\n",
            "Speed: 3.6ms preprocess, 31.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595132195.jpg: 640x480 1 Bags, 26.1ms\n",
            "Speed: 4.3ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607899365.jpg: 640x480 2 Tabless, 24.5ms\n",
            "Speed: 3.7ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462622397.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 2.1ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465060524.jpg: 640x512 (no detections), 26.1ms\n",
            "Speed: 4.3ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29613754823.jpg: 640x384 1 Chairs, 21.1ms\n",
            "Speed: 2.5ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36463687086.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 2.8ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29548683079.jpg: 640x480 1 Tables, 24.9ms\n",
            "Speed: 3.9ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591877000.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 3.5ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470413255.jpg: 480x640 1 Bags, 25.4ms\n",
            "Speed: 8.5ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462694900.jpg: 640x480 3 Bagss, 2 Clothes_baby_girlss, 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484500744.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.0ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486733602.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458355551.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.2ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484940738.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 3.5ms preprocess, 23.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467315296.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 4.3ms preprocess, 25.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463883977.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.8ms preprocess, 24.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462065095.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590671416.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 3.0ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467545010.jpg: 640x608 1 Bags, 32.2ms\n",
            "Speed: 5.1ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36464819979.jpg: 640x480 (no detections), 25.7ms\n",
            "Speed: 4.2ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463521569.jpg: 640x480 1 Bags, 25.4ms\n",
            "Speed: 3.9ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456724145.jpg: 640x384 1 Clothes_baby_girls, 21.3ms\n",
            "Speed: 3.2ms preprocess, 21.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36457062152.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.2ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29646658251.jpg: 640x480 1 Tables, 23.2ms\n",
            "Speed: 4.2ms preprocess, 23.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456617465.jpg: 512x640 1 Bags, 27.3ms\n",
            "Speed: 3.7ms preprocess, 27.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458381663.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 3.3ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457750758.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 4.1ms preprocess, 24.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479703972.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 3.6ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592295079.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 3.7ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457180790.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.0ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476509089.jpg: 640x448 (no detections), 25.3ms\n",
            "Speed: 3.5ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37593603317.jpg: 640x640 1 Bags, 32.4ms\n",
            "Speed: 4.5ms preprocess, 32.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459943619.jpg: 640x384 1 Bags, 22.6ms\n",
            "Speed: 3.4ms preprocess, 22.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29622693834.jpg: 480x640 1 Tables, 25.8ms\n",
            "Speed: 3.7ms preprocess, 25.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463767213.jpg: 640x640 (no detections), 32.1ms\n",
            "Speed: 4.0ms preprocess, 32.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29615716706.jpg: 640x640 1 Tables, 34.4ms\n",
            "Speed: 3.1ms preprocess, 34.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598871573.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 4.2ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472222978.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 4.0ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587010664.jpg: 448x640 1 Bags, 27.8ms\n",
            "Speed: 3.1ms preprocess, 27.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458058922.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 4.2ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473815039.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 3.7ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594836960.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 2.6ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463320384.jpg: 640x384 (no detections), 21.4ms\n",
            "Speed: 3.3ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36465329508.jpg: 640x480 1 Clothes_baby_girls, 23.9ms\n",
            "Speed: 4.1ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467049943.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 4.2ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471583935.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591888669.jpg: 640x544 2 Clothes_baby_girlss, 32.2ms\n",
            "Speed: 4.7ms preprocess, 32.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36457142763.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 5.2ms preprocess, 24.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596770373.jpg: 640x384 2 Clothes_baby_girlss, 21.9ms\n",
            "Speed: 3.3ms preprocess, 21.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37596706541.jpg: 640x480 1 Clothes_baby_girls, 28.7ms\n",
            "Speed: 3.9ms preprocess, 28.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480711847.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 6.8ms preprocess, 23.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464257693.jpg: 640x544 1 Clothes_baby_girls, 32.7ms\n",
            "Speed: 3.9ms preprocess, 32.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37590019475.jpg: 512x640 (no detections), 30.5ms\n",
            "Speed: 3.4ms preprocess, 30.5ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484579556.jpg: 640x480 1 Bags, 26.6ms\n",
            "Speed: 4.9ms preprocess, 26.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461755961.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 5.7ms preprocess, 23.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468322229.jpg: 640x448 1 Clothes_baby_girls, 23.5ms\n",
            "Speed: 3.8ms preprocess, 23.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36471018135.jpg: 480x640 (no detections), 26.7ms\n",
            "Speed: 3.9ms preprocess, 26.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456103894.jpg: 640x416 1 Clothes_baby_girls, 24.3ms\n",
            "Speed: 2.5ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36457565092.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 4.0ms preprocess, 24.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461864805.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 4.2ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459480029.jpg: 480x640 1 Clothes_baby_girls, 26.5ms\n",
            "Speed: 4.1ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456307616.jpg: 640x480 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 3.9ms preprocess, 24.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480947695.jpg: 640x384 1 Bags, 20.7ms\n",
            "Speed: 5.4ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37588019530.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 3.6ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465237797.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29649214076.jpg: 640x640 1 Tables, 31.9ms\n",
            "Speed: 5.4ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461033088.jpg: 640x480 (no detections), 27.0ms\n",
            "Speed: 4.2ms preprocess, 27.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463364333.jpg: 640x640 1 Bags, 36.0ms\n",
            "Speed: 7.3ms preprocess, 36.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595577304.jpg: 640x480 1 Clothes_baby_girls, 25.6ms\n",
            "Speed: 4.1ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29642682904.jpg: 640x480 1 Chairs, 24.1ms\n",
            "Speed: 2.7ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587314888.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 3.4ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483933275.jpg: 640x480 2 Bagss, 22.9ms\n",
            "Speed: 4.0ms preprocess, 22.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29619177909.jpg: 480x640 1 Tables, 26.3ms\n",
            "Speed: 4.7ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36481584048.jpg: 640x480 1 Bags, 30.8ms\n",
            "Speed: 6.3ms preprocess, 30.8ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589114244.jpg: 640x480 1 Bags, 22.3ms\n",
            "Speed: 3.9ms preprocess, 22.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29611147357.jpg: 640x480 1 Chairs, 24.8ms\n",
            "Speed: 4.7ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471473663.jpg: 640x640 1 Bags, 1 Clothes_baby_girls, 35.1ms\n",
            "Speed: 6.6ms preprocess, 35.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36471851689.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.1ms preprocess, 24.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459025258.jpg: 640x320 1 Bags, 24.9ms\n",
            "Speed: 2.6ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36479572950.jpg: 640x480 (no detections), 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479507117.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 9.0ms preprocess, 23.7ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459449189.jpg: 640x448 1 Bags, 24.3ms\n",
            "Speed: 3.8ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37587293829.jpg: 608x640 1 Clothes_baby_girls, 31.2ms\n",
            "Speed: 5.0ms preprocess, 31.2ms inference, 1.7ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462722734.jpg: 640x384 1 Bags, 22.8ms\n",
            "Speed: 3.8ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37589212329.jpg: 640x608 1 Bags, 35.3ms\n",
            "Speed: 4.4ms preprocess, 35.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36474472482.jpg: 640x480 (no detections), 24.8ms\n",
            "Speed: 6.1ms preprocess, 24.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455169929.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 4.1ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462363891.jpg: 384x640 (no detections), 22.2ms\n",
            "Speed: 2.2ms preprocess, 22.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36480031032.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 2.6ms preprocess, 25.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29623361605.jpg: 640x512 2 Chairss, 27.5ms\n",
            "Speed: 4.2ms preprocess, 27.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36471969122.jpg: 640x640 1 Bags, 32.2ms\n",
            "Speed: 5.5ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458257787.jpg: 640x480 1 Bags, 26.8ms\n",
            "Speed: 4.0ms preprocess, 26.8ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457379663.jpg: 640x480 (no detections), 23.6ms\n",
            "Speed: 4.0ms preprocess, 23.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460463179.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 5.9ms preprocess, 25.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596231421.jpg: 640x480 (no detections), 27.2ms\n",
            "Speed: 3.9ms preprocess, 27.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616218347.jpg: 640x480 1 Tables, 24.1ms\n",
            "Speed: 3.9ms preprocess, 24.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485901646.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 3.9ms preprocess, 23.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459176398.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.4ms preprocess, 23.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587502657.jpg: 640x544 (no detections), 34.3ms\n",
            "Speed: 10.2ms preprocess, 34.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37590728092.jpg: 640x480 1 Clothes_baby_girls, 26.0ms\n",
            "Speed: 3.9ms preprocess, 26.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590337017.jpg: 640x384 (no detections), 21.3ms\n",
            "Speed: 3.1ms preprocess, 21.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29644913229.jpg: 640x480 1 Chairs, 26.5ms\n",
            "Speed: 5.9ms preprocess, 26.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589564009.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 4.6ms preprocess, 23.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592528532.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 7.7ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598616205.jpg: 480x640 (no detections), 25.5ms\n",
            "Speed: 9.7ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457165135.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 4.0ms preprocess, 24.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477251031.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 4.2ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458475883.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.7ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37575829832.jpg: 640x416 (no detections), 26.8ms\n",
            "Speed: 3.5ms preprocess, 26.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36462827263.jpg: 640x288 1 Clothes_baby_girls, 19.8ms\n",
            "Speed: 5.8ms preprocess, 19.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36465299434.jpg: 640x640 1 Bags, 32.5ms\n",
            "Speed: 5.6ms preprocess, 32.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37586387174.jpg: 320x640 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485617537.jpg: 640x512 1 Bags, 24.3ms\n",
            "Speed: 4.5ms preprocess, 24.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29641126315.jpg: 640x480 1 Tables, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486837774.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 4.1ms preprocess, 23.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29652310369.jpg: 640x288 1 Chairs, 1 Tables, 25.0ms\n",
            "Speed: 2.6ms preprocess, 25.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36457625547.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 6.5ms preprocess, 24.2ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460980050.jpg: 640x480 1 Bags, 26.2ms\n",
            "Speed: 2.8ms preprocess, 26.2ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485463940.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 4.8ms preprocess, 23.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477332147.jpg: 640x480 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 6.8ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592946669.jpg: 640x448 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 5.6ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/29640183598.jpg: 640x480 1 Chairs, 24.6ms\n",
            "Speed: 3.9ms preprocess, 24.6ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29609547768.jpg: 576x640 1 Tables, 42.2ms\n",
            "Speed: 4.1ms preprocess, 42.2ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588952959.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 12.4ms preprocess, 24.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459543236.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 8.1ms preprocess, 23.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469433339.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 7.2ms preprocess, 23.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463567125.jpg: 640x448 (no detections), 25.4ms\n",
            "Speed: 4.4ms preprocess, 25.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36478515525.jpg: 640x384 (no detections), 21.7ms\n",
            "Speed: 2.7ms preprocess, 21.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36474729150.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37599016349.jpg: 640x512 (no detections), 28.4ms\n",
            "Speed: 4.0ms preprocess, 28.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36415847729.jpg: 640x480 1 Clothes_baby_girls, 33.0ms\n",
            "Speed: 3.9ms preprocess, 33.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29632435818.jpg: 320x640 (no detections), 23.6ms\n",
            "Speed: 1.5ms preprocess, 23.6ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597117580.jpg: 640x576 1 Bags, 32.2ms\n",
            "Speed: 4.9ms preprocess, 32.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36462202124.jpg: 640x512 1 Clothes_baby_girls, 26.6ms\n",
            "Speed: 4.3ms preprocess, 26.6ms inference, 7.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36482839316.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 8.1ms preprocess, 24.8ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587482061.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.0ms preprocess, 23.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458990081.jpg: 640x640 1 Bags, 34.2ms\n",
            "Speed: 3.2ms preprocess, 34.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464174719.jpg: 640x384 1 Bags, 21.5ms\n",
            "Speed: 3.4ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36481872654.jpg: 384x640 1 Clothes_baby_girls, 23.5ms\n",
            "Speed: 8.2ms preprocess, 23.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29635733309.jpg: 640x480 1 Chairs, 27.5ms\n",
            "Speed: 4.3ms preprocess, 27.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29649337424.jpg: 640x640 1 Chairs, 35.1ms\n",
            "Speed: 6.1ms preprocess, 35.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474325505.jpg: 640x576 1 Bags, 32.4ms\n",
            "Speed: 10.3ms preprocess, 32.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36471108125.jpg: 640x512 1 Bags, 25.2ms\n",
            "Speed: 4.7ms preprocess, 25.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36486968203.jpg: 640x480 1 Bags, 29.4ms\n",
            "Speed: 4.4ms preprocess, 29.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595083330.jpg: 640x320 1 Bags, 19.8ms\n",
            "Speed: 2.6ms preprocess, 19.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36459418651.jpg: 480x640 (no detections), 27.1ms\n",
            "Speed: 5.6ms preprocess, 27.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29648066754.jpg: 288x640 1 Tables, 26.6ms\n",
            "Speed: 3.8ms preprocess, 26.6ms inference, 2.0ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590484329.jpg: 480x640 (no detections), 27.2ms\n",
            "Speed: 3.7ms preprocess, 27.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36481561822.jpg: 640x288 (no detections), 20.6ms\n",
            "Speed: 2.5ms preprocess, 20.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36481413080.jpg: 640x480 (no detections), 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464856282.jpg: 640x384 1 Bags, 20.6ms\n",
            "Speed: 5.5ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29652620125.jpg: 640x480 1 Tables, 25.3ms\n",
            "Speed: 6.3ms preprocess, 25.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598180081.jpg: 640x640 1 Bags, 34.7ms\n",
            "Speed: 3.9ms preprocess, 34.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29638914394.jpg: 640x480 (no detections), 25.4ms\n",
            "Speed: 5.0ms preprocess, 25.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598383830.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 4.3ms preprocess, 24.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474506244.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.2ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465189965.jpg: 480x640 (no detections), 25.8ms\n",
            "Speed: 4.5ms preprocess, 25.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29605119449.jpg: 640x480 1 Chairs, 24.8ms\n",
            "Speed: 4.4ms preprocess, 24.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593051309.jpg: 640x480 1 Bags, 23.0ms\n",
            "Speed: 4.3ms preprocess, 23.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29631512456.jpg: 640x480 1 Tables, 23.6ms\n",
            "Speed: 3.2ms preprocess, 23.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475821773.jpg: 640x384 1 Bags, 23.9ms\n",
            "Speed: 3.4ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36479207506.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 5.8ms preprocess, 24.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481756523.jpg: 640x640 1 Clothes_baby_girls, 34.2ms\n",
            "Speed: 5.1ms preprocess, 34.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29606049652.jpg: 640x480 1 Chairs, 1 Tables, 24.9ms\n",
            "Speed: 4.5ms preprocess, 24.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588059297.jpg: 640x480 1 Bags, 22.5ms\n",
            "Speed: 7.7ms preprocess, 22.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616727365.jpg: 640x480 2 Chairss, 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463575464.jpg: 640x544 1 Bags, 36.8ms\n",
            "Speed: 5.8ms preprocess, 36.8ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37587868234.jpg: 640x288 (no detections), 27.6ms\n",
            "Speed: 4.8ms preprocess, 27.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36482397093.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 8.1ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460140680.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 6.0ms preprocess, 24.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598942058.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.5ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598859315.jpg: 640x480 (no detections), 33.8ms\n",
            "Speed: 4.4ms preprocess, 33.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466769953.jpg: 640x480 1 Bags, 30.8ms\n",
            "Speed: 5.4ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458565029.jpg: 512x640 (no detections), 31.9ms\n",
            "Speed: 4.7ms preprocess, 31.9ms inference, 2.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474433618.jpg: 512x640 (no detections), 25.0ms\n",
            "Speed: 4.7ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588870752.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 8.2ms preprocess, 25.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456659050.jpg: 640x384 1 Clothes_baby_girls, 21.2ms\n",
            "Speed: 7.1ms preprocess, 21.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37593915186.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 3.1ms preprocess, 25.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589720347.jpg: 480x640 1 Bags, 26.7ms\n",
            "Speed: 4.3ms preprocess, 26.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463650580.jpg: 640x480 1 Chairs, 26.9ms\n",
            "Speed: 2.6ms preprocess, 26.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475729208.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.0ms preprocess, 23.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590134636.jpg: 640x576 1 Bags, 31.2ms\n",
            "Speed: 8.9ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/37598317104.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 3.2ms preprocess, 25.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463372216.jpg: 640x288 1 Clothes_baby_girls, 20.7ms\n",
            "Speed: 3.1ms preprocess, 20.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36480455258.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 2.4ms preprocess, 24.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597125496.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 2.8ms preprocess, 23.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458751386.jpg: 640x608 1 Bags, 33.8ms\n",
            "Speed: 4.7ms preprocess, 33.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36464253578.jpg: 640x480 1 Clothes_baby_girls, 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475710168.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 4.2ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29623912821.jpg: 640x384 1 Chairs, 23.2ms\n",
            "Speed: 2.2ms preprocess, 23.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36457684725.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 2.5ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595963481.jpg: 640x544 1 Clothes_baby_girls, 31.8ms\n",
            "Speed: 5.4ms preprocess, 31.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36482513559.jpg: 480x640 (no detections), 26.7ms\n",
            "Speed: 2.6ms preprocess, 26.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29621165712.jpg: 640x480 1 Tables, 25.4ms\n",
            "Speed: 2.7ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479716948.jpg: 448x640 2 Clothes_baby_girlss, 27.5ms\n",
            "Speed: 3.2ms preprocess, 27.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29616108853.jpg: 480x640 1 Chairs, 25.9ms\n",
            "Speed: 2.4ms preprocess, 25.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36481612427.jpg: 640x384 1 Bags, 21.4ms\n",
            "Speed: 3.4ms preprocess, 21.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36463548593.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484404976.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 3.3ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588171069.jpg: 416x640 (no detections), 25.2ms\n",
            "Speed: 2.4ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29616871319.jpg: 640x480 1 Tables, 24.3ms\n",
            "Speed: 4.6ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591880432.jpg: 480x640 1 Clothes_baby_girls, 25.6ms\n",
            "Speed: 3.1ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459655289.jpg: 640x640 1 Bags, 34.1ms\n",
            "Speed: 7.2ms preprocess, 34.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469317867.jpg: 640x480 1 Bags, 26.3ms\n",
            "Speed: 5.8ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459781241.jpg: 640x480 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 4.9ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479441512.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 3.7ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588250298.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 3.9ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589912328.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.5ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466806986.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 2.5ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29646992576.jpg: 640x384 1 Tables, 22.1ms\n",
            "Speed: 3.8ms preprocess, 22.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36486203522.jpg: 480x640 (no detections), 26.3ms\n",
            "Speed: 2.5ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460547053.jpg: 608x640 1 Bags, 31.7ms\n",
            "Speed: 4.2ms preprocess, 31.7ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474206871.jpg: 480x640 1 Clothes_baby_girls, 26.6ms\n",
            "Speed: 4.3ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589078884.jpg: 640x480 1 Clothes_baby_girls, 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462141140.jpg: 640x384 2 Clothes_baby_girlss, 21.1ms\n",
            "Speed: 3.3ms preprocess, 21.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36459848062.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 4.2ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462690661.jpg: 480x640 1 Bags, 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29617365607.jpg: 640x320 1 Tables, 21.7ms\n",
            "Speed: 2.3ms preprocess, 21.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36486720394.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.8ms preprocess, 24.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37599018808.jpg: 480x640 1 Bags, 27.5ms\n",
            "Speed: 2.7ms preprocess, 27.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457739757.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457641954.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 6.4ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460167862.jpg: 512x640 1 Bags, 25.8ms\n",
            "Speed: 3.1ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464598451.jpg: 640x384 1 Bags, 20.7ms\n",
            "Speed: 5.7ms preprocess, 20.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36458577557.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 2.5ms preprocess, 23.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29610425934.jpg: 640x480 2 Chairss, 24.5ms\n",
            "Speed: 3.8ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29647475642.jpg: 416x640 1 Tables, 26.3ms\n",
            "Speed: 2.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484805112.jpg: 480x640 1 Bags, 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36475841452.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 4.1ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463526188.jpg: 512x640 1 Bags, 26.1ms\n",
            "Speed: 6.5ms preprocess, 26.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458105431.jpg: 640x480 1 Bags, 26.8ms\n",
            "Speed: 3.4ms preprocess, 26.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589443032.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589631910.jpg: 640x288 1 Clothes_baby_girls, 19.7ms\n",
            "Speed: 2.9ms preprocess, 19.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36460276122.jpg: 480x640 (no detections), 25.9ms\n",
            "Speed: 2.4ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462777092.jpg: 640x544 1 Bags, 30.7ms\n",
            "Speed: 2.9ms preprocess, 30.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36464808437.jpg: 640x480 (no detections), 25.9ms\n",
            "Speed: 3.7ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461174794.jpg: 576x640 (no detections), 30.2ms\n",
            "Speed: 3.1ms preprocess, 30.2ms inference, 0.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462537150.jpg: 640x384 1 Bags, 27.7ms\n",
            "Speed: 3.3ms preprocess, 27.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36435241961.jpg: 640x640 1 Clothes_baby_girls, 31.0ms\n",
            "Speed: 3.3ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36481999134.jpg: 448x640 1 Bags, 28.6ms\n",
            "Speed: 3.4ms preprocess, 28.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486375444.jpg: 640x480 1 Clothes_baby_girls, 26.5ms\n",
            "Speed: 3.4ms preprocess, 26.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464300643.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.1ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29626903364.jpg: 640x480 1 Chairs, 24.0ms\n",
            "Speed: 3.8ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466048683.jpg: 640x384 (no detections), 21.1ms\n",
            "Speed: 2.9ms preprocess, 21.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37587204128.jpg: 640x512 1 Bags, 25.7ms\n",
            "Speed: 4.3ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37598484850.jpg: 640x480 1 Bags, 27.0ms\n",
            "Speed: 2.7ms preprocess, 27.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589243421.jpg: 320x640 1 Clothes_baby_girls, 19.6ms\n",
            "Speed: 1.9ms preprocess, 19.6ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587702108.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 3.3ms preprocess, 23.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587887980.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 4.3ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460959484.jpg: 640x384 1 Clothes_baby_girls, 21.4ms\n",
            "Speed: 3.6ms preprocess, 21.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36457189902.jpg: 512x640 1 Clothes_baby_girls, 26.7ms\n",
            "Speed: 3.7ms preprocess, 26.7ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29642413339.jpg: 640x480 1 Chairs, 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468804481.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29609335695.jpg: 288x640 1 Tables, 20.0ms\n",
            "Speed: 2.7ms preprocess, 20.0ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455875105.jpg: 640x448 1 Bags, 24.1ms\n",
            "Speed: 2.7ms preprocess, 24.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36471536115.jpg: 640x640 1 Bags, 32.9ms\n",
            "Speed: 3.4ms preprocess, 32.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461640122.jpg: 640x448 1 Bags, 25.2ms\n",
            "Speed: 2.4ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36464762927.jpg: 640x480 1 Clothes_baby_girls, 25.7ms\n",
            "Speed: 2.4ms preprocess, 25.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469987273.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.1ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462510925.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469660193.jpg: 640x384 1 Bags, 21.7ms\n",
            "Speed: 4.0ms preprocess, 21.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36466313720.jpg: 640x640 (no detections), 31.3ms\n",
            "Speed: 2.7ms preprocess, 31.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36476253463.jpg: 640x384 (no detections), 22.9ms\n",
            "Speed: 3.4ms preprocess, 22.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36466991634.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 3.9ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460350526.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 2.5ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589044768.jpg: 640x640 1 Bags, 32.8ms\n",
            "Speed: 3.4ms preprocess, 32.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597521124.jpg: 640x480 (no detections), 26.8ms\n",
            "Speed: 4.2ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461445675.jpg: 640x640 1 Bags, 32.1ms\n",
            "Speed: 3.1ms preprocess, 32.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479208796.jpg: 640x480 1 Bags, 26.6ms\n",
            "Speed: 3.5ms preprocess, 26.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485156631.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 2.4ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588929111.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 3.9ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620351450.jpg: 640x480 1 Tables, 23.6ms\n",
            "Speed: 6.7ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464642444.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 4.3ms preprocess, 23.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478105850.jpg: 640x320 1 Clothes_baby_girls, 20.7ms\n",
            "Speed: 2.8ms preprocess, 20.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36474931912.jpg: 640x384 1 Bags, 20.9ms\n",
            "Speed: 3.3ms preprocess, 20.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37593689648.jpg: 480x640 (no detections), 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593342064.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 4.4ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469684606.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 2.7ms preprocess, 25.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37599025745.jpg: 640x640 1 Clothes_baby_girls, 33.5ms\n",
            "Speed: 4.3ms preprocess, 33.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590305389.jpg: 640x480 1 Bags, 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473180506.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 3.9ms preprocess, 23.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458809512.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 3.1ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588582071.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.0ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29646448266.jpg: 608x640 (no detections), 34.1ms\n",
            "Speed: 5.0ms preprocess, 34.1ms inference, 0.6ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462719432.jpg: 640x640 1 Bags, 33.6ms\n",
            "Speed: 3.5ms preprocess, 33.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479659333.jpg: 640x640 1 Bags, 32.8ms\n",
            "Speed: 4.1ms preprocess, 32.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36390114091.jpg: 640x512 1 Bags, 26.4ms\n",
            "Speed: 3.8ms preprocess, 26.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29636226443.jpg: 640x480 1 Tables, 27.2ms\n",
            "Speed: 3.5ms preprocess, 27.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467281032.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 2.5ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29643808604.jpg: 640x480 1 Tables, 25.2ms\n",
            "Speed: 4.1ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483137045.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 3.7ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588398983.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 3.5ms preprocess, 25.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482726216.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 3.8ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589461810.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 2.4ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486417703.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29624180316.jpg: 640x480 1 Tables, 23.6ms\n",
            "Speed: 4.1ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466775148.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.4ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483930426.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 3.0ms preprocess, 24.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460033839.jpg: 640x384 1 Tables, 21.2ms\n",
            "Speed: 2.6ms preprocess, 21.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36455829527.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 3.1ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598015953.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.8ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29638777071.jpg: 640x480 1 Tables, 24.9ms\n",
            "Speed: 4.2ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29622112783.jpg: 480x640 1 Tables, 26.1ms\n",
            "Speed: 4.2ms preprocess, 26.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457297676.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.3ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484337734.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463010510.jpg: 640x288 1 Clothes_baby_girls, 19.7ms\n",
            "Speed: 2.2ms preprocess, 19.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36471756744.jpg: 640x480 (no detections), 26.3ms\n",
            "Speed: 2.8ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588072189.jpg: 640x512 1 Bags, 23.8ms\n",
            "Speed: 4.2ms preprocess, 23.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37596132344.jpg: 480x640 (no detections), 25.7ms\n",
            "Speed: 2.5ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456847962.jpg: 480x640 1 Bags, 25.5ms\n",
            "Speed: 2.6ms preprocess, 25.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37592763317.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.1ms preprocess, 25.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29615899982.jpg: 640x480 (no detections), 24.6ms\n",
            "Speed: 3.1ms preprocess, 24.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464058805.jpg: 640x512 1 Bags, 26.0ms\n",
            "Speed: 4.5ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36461652284.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457480933.jpg: 640x480 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 3.3ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458351901.jpg: 640x480 (no detections), 26.6ms\n",
            "Speed: 3.5ms preprocess, 26.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464149275.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460137092.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 3.9ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468072800.jpg: 640x480 (no detections), 24.6ms\n",
            "Speed: 4.3ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596150083.jpg: 640x640 1 Bags, 32.8ms\n",
            "Speed: 5.2ms preprocess, 32.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466255816.jpg: 640x480 1 Bags, 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464894957.jpg: 512x640 (no detections), 26.9ms\n",
            "Speed: 3.1ms preprocess, 26.9ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36473185025.jpg: 640x480 1 Bags, 25.6ms\n",
            "Speed: 5.0ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458765715.jpg: 640x640 1 Bags, 32.8ms\n",
            "Speed: 5.7ms preprocess, 32.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460722094.jpg: 640x480 (no detections), 25.5ms\n",
            "Speed: 3.7ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587349793.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 2.6ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36468394732.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.1ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587017851.jpg: 640x448 1 Bags, 24.2ms\n",
            "Speed: 3.6ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36476535738.jpg: 544x640 1 Bags, 30.3ms\n",
            "Speed: 3.3ms preprocess, 30.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456585405.jpg: 640x480 1 Bags, 25.4ms\n",
            "Speed: 3.2ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588279198.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 2.7ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455337875.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.0ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485831237.jpg: 640x448 1 Bags, 25.1ms\n",
            "Speed: 2.6ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36460150742.jpg: 640x640 1 Bags, 33.1ms\n",
            "Speed: 5.4ms preprocess, 33.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588061832.jpg: 640x480 1 Clothes_baby_girls, 26.6ms\n",
            "Speed: 4.2ms preprocess, 26.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588461894.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.8ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463510939.jpg: 640x480 1 Clothes_baby_girls, 23.7ms\n",
            "Speed: 3.9ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29647928588.jpg: 480x640 1 Tables, 26.4ms\n",
            "Speed: 2.5ms preprocess, 26.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29616721912.jpg: 480x640 1 Tables, 25.9ms\n",
            "Speed: 3.0ms preprocess, 25.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597747615.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.1ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461343643.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 4.1ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471421410.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 4.9ms preprocess, 23.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455872670.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 5.0ms preprocess, 23.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486201460.jpg: 640x480 2 Clothes_baby_girlss, 22.9ms\n",
            "Speed: 2.5ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459766868.jpg: 480x640 1 Clothes_baby_girls, 27.7ms\n",
            "Speed: 2.5ms preprocess, 27.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597492585.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 3.6ms preprocess, 25.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589412945.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 3.2ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588349577.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 3.7ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29644847725.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 2.5ms preprocess, 23.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461002367.jpg: 640x480 (no detections), 24.4ms\n",
            "Speed: 3.8ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588871768.jpg: 480x640 1 Clothes_baby_girls, 25.8ms\n",
            "Speed: 2.5ms preprocess, 25.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462334996.jpg: 640x512 1 Clothes_baby_girls, 25.2ms\n",
            "Speed: 3.5ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36456973604.jpg: 480x640 1 Bags, 26.7ms\n",
            "Speed: 2.5ms preprocess, 26.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36483129800.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 3.6ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459285245.jpg: 480x640 1 Bags, 26.0ms\n",
            "Speed: 2.5ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29610427092.jpg: 448x640 1 Tables, 26.3ms\n",
            "Speed: 2.5ms preprocess, 26.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36476789955.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 2.6ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465986833.jpg: 576x640 1 Bags, 32.5ms\n",
            "Speed: 4.6ms preprocess, 32.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29608216965.jpg: 640x480 2 Chairss, 25.5ms\n",
            "Speed: 2.9ms preprocess, 25.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484641630.jpg: 640x320 (no detections), 19.6ms\n",
            "Speed: 2.9ms preprocess, 19.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36485465496.jpg: 640x320 1 Bags, 18.7ms\n",
            "Speed: 4.4ms preprocess, 18.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37597952736.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 4.4ms preprocess, 23.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476187644.jpg: 576x640 1 Bags, 31.7ms\n",
            "Speed: 5.4ms preprocess, 31.7ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36480048209.jpg: 480x640 1 Bags, 27.8ms\n",
            "Speed: 4.0ms preprocess, 27.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457500253.jpg: 480x640 1 Bags, 24.8ms\n",
            "Speed: 2.6ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36476379004.jpg: 640x480 (no detections), 24.8ms\n",
            "Speed: 3.6ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456626051.jpg: 480x640 (no detections), 26.2ms\n",
            "Speed: 2.5ms preprocess, 26.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479379543.jpg: 640x384 1 Bags, 21.5ms\n",
            "Speed: 3.4ms preprocess, 21.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29645437537.jpg: 288x640 (no detections), 18.4ms\n",
            "Speed: 2.9ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460258310.jpg: 480x640 1 Bags, 24.7ms\n",
            "Speed: 3.0ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466950670.jpg: 640x480 1 Bags, 26.6ms\n",
            "Speed: 5.4ms preprocess, 26.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589277792.jpg: 640x640 1 Bags, 33.4ms\n",
            "Speed: 5.2ms preprocess, 33.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459864330.jpg: 640x640 1 Bags, 32.9ms\n",
            "Speed: 4.5ms preprocess, 32.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459423246.jpg: 640x480 1 Bags, 26.1ms\n",
            "Speed: 3.8ms preprocess, 26.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477742854.jpg: 640x480 1 Clothes_baby_girls, 23.8ms\n",
            "Speed: 4.2ms preprocess, 23.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29640746441.jpg: 640x320 1 Tables, 21.0ms\n",
            "Speed: 3.1ms preprocess, 21.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37594263754.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.3ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464034695.jpg: 640x384 1 Bags, 21.3ms\n",
            "Speed: 3.3ms preprocess, 21.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462987798.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463211722.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 2.5ms preprocess, 23.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620277051.jpg: 640x384 1 Chairs, 22.5ms\n",
            "Speed: 4.2ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37589617349.jpg: 480x640 (no detections), 26.4ms\n",
            "Speed: 3.7ms preprocess, 26.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486547572.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 4.4ms preprocess, 25.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29615045600.jpg: 640x640 1 Tables, 35.4ms\n",
            "Speed: 4.0ms preprocess, 35.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463837031.jpg: 640x480 1 Bags, 25.6ms\n",
            "Speed: 7.5ms preprocess, 25.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463720978.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 3.6ms preprocess, 23.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459088222.jpg: 480x640 1 Bags, 27.2ms\n",
            "Speed: 4.2ms preprocess, 27.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460603776.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 4.2ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469135620.jpg: 640x384 1 Clothes_baby_girls, 20.2ms\n",
            "Speed: 3.2ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37590131360.jpg: 640x480 1 Bags, 26.4ms\n",
            "Speed: 3.9ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463974521.jpg: 480x640 (no detections), 24.8ms\n",
            "Speed: 4.0ms preprocess, 24.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29619475542.jpg: 384x640 (no detections), 23.5ms\n",
            "Speed: 2.2ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29638460249.jpg: 640x640 1 Tables, 31.6ms\n",
            "Speed: 10.2ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465857669.jpg: 640x480 1 Bags, 33.1ms\n",
            "Speed: 4.0ms preprocess, 33.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479831253.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 4.0ms preprocess, 23.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472051211.jpg: 640x480 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 4.3ms preprocess, 23.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461782002.jpg: 608x640 1 Tables, 33.5ms\n",
            "Speed: 4.6ms preprocess, 33.5ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29625931944.jpg: 640x320 1 Tables, 21.8ms\n",
            "Speed: 2.8ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36458780095.jpg: 480x640 (no detections), 25.5ms\n",
            "Speed: 4.1ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479987515.jpg: 640x480 1 Bags, 26.3ms\n",
            "Speed: 4.1ms preprocess, 26.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457153268.jpg: 512x640 (no detections), 26.0ms\n",
            "Speed: 8.6ms preprocess, 26.0ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588525872.jpg: 640x480 1 Clothes_baby_girls, 25.6ms\n",
            "Speed: 4.0ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477211370.jpg: 640x480 1 Clothes_baby_girls, 24.2ms\n",
            "Speed: 4.0ms preprocess, 24.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459682361.jpg: 640x480 1 Clothes_baby_girls, 23.1ms\n",
            "Speed: 4.0ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597491044.jpg: 480x640 (no detections), 29.9ms\n",
            "Speed: 4.0ms preprocess, 29.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29624480328.jpg: 288x640 1 Chairs, 1 Tables, 22.1ms\n",
            "Speed: 2.7ms preprocess, 22.1ms inference, 1.8ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29604240874.jpg: 640x480 (no detections), 25.8ms\n",
            "Speed: 3.9ms preprocess, 25.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476656379.jpg: 480x640 1 Tables, 29.4ms\n",
            "Speed: 4.4ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29613595400.jpg: 640x352 1 Chairs, 23.7ms\n",
            "Speed: 2.0ms preprocess, 23.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 352)\n",
            "\n",
            "image 1/1 /content/train_data/36474670776.jpg: 640x608 1 Bags, 33.3ms\n",
            "Speed: 12.2ms preprocess, 33.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/37596741560.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 4.3ms preprocess, 25.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29626340026.jpg: 640x512 1 Chairs, 25.8ms\n",
            "Speed: 4.2ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29649068613.jpg: 480x640 1 Tables, 31.2ms\n",
            "Speed: 4.1ms preprocess, 31.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590604541.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589227156.jpg: 640x384 1 Clothes_baby_girls, 22.5ms\n",
            "Speed: 3.7ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36461465527.jpg: 640x480 (no detections), 27.2ms\n",
            "Speed: 2.6ms preprocess, 27.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589241008.jpg: 480x640 (no detections), 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485756373.jpg: 640x480 (no detections), 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459531264.jpg: 480x640 (no detections), 26.6ms\n",
            "Speed: 4.1ms preprocess, 26.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588149458.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 6.8ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474302423.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 2.8ms preprocess, 22.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475295429.jpg: 640x288 1 Bags, 24.9ms\n",
            "Speed: 2.5ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36463354212.jpg: 640x384 1 Bags, 20.3ms\n",
            "Speed: 5.7ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37597376027.jpg: 576x640 1 Bags, 29.2ms\n",
            "Speed: 5.4ms preprocess, 29.2ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598045027.jpg: 480x640 2 Bagss, 25.8ms\n",
            "Speed: 4.2ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486323119.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.0ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483277438.jpg: 576x640 1 Bags, 30.7ms\n",
            "Speed: 4.8ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462366951.jpg: 640x480 1 Clothes_baby_girls, 26.9ms\n",
            "Speed: 4.2ms preprocess, 26.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597435328.jpg: 640x544 1 Bags, 33.0ms\n",
            "Speed: 4.6ms preprocess, 33.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36486052861.jpg: 640x384 1 Bags, 21.6ms\n",
            "Speed: 3.3ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37598445862.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 4.0ms preprocess, 26.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29649406290.jpg: 640x448 2 Chairss, 1 Tables, 25.1ms\n",
            "Speed: 3.7ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/29631900669.jpg: 640x480 1 Tables, 24.6ms\n",
            "Speed: 3.9ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588193772.jpg: 480x640 (no detections), 27.4ms\n",
            "Speed: 4.2ms preprocess, 27.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465309089.jpg: 640x480 (no detections), 25.4ms\n",
            "Speed: 9.4ms preprocess, 25.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478687141.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.3ms preprocess, 23.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464494005.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 4.4ms preprocess, 23.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459523267.jpg: 640x480 (no detections), 31.8ms\n",
            "Speed: 4.1ms preprocess, 31.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466095152.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.0ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461456864.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 6.3ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29635984300.jpg: 640x480 1 Tables, 23.9ms\n",
            "Speed: 4.5ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467062681.jpg: 640x480 1 Clothes_baby_girls, 23.8ms\n",
            "Speed: 8.2ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457394197.jpg: 640x544 1 Bags, 31.5ms\n",
            "Speed: 6.4ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37595645040.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29631355715.jpg: 640x480 1 Tables, 24.9ms\n",
            "Speed: 4.8ms preprocess, 24.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36243494022.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 3.5ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467013224.jpg: 640x480 1 Bags, 23.0ms\n",
            "Speed: 6.9ms preprocess, 23.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482182501.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 5.4ms preprocess, 23.2ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469631840.jpg: 480x640 (no detections), 25.7ms\n",
            "Speed: 4.8ms preprocess, 25.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469652391.jpg: 640x480 (no detections), 30.5ms\n",
            "Speed: 3.2ms preprocess, 30.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461833548.jpg: 640x480 (no detections), 24.3ms\n",
            "Speed: 6.4ms preprocess, 24.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459212568.jpg: 640x512 (no detections), 25.8ms\n",
            "Speed: 4.9ms preprocess, 25.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36484846244.jpg: 640x480 1 Bags, 25.5ms\n",
            "Speed: 4.7ms preprocess, 25.5ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460906978.jpg: 640x480 (no detections), 27.0ms\n",
            "Speed: 5.2ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29634454149.jpg: 640x480 1 Chairs, 28.6ms\n",
            "Speed: 4.8ms preprocess, 28.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592893981.jpg: 512x640 (no detections), 27.5ms\n",
            "Speed: 4.8ms preprocess, 27.5ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479203853.jpg: 640x480 3 Clothes_baby_girlss, 24.0ms\n",
            "Speed: 4.2ms preprocess, 24.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455691299.jpg: 640x640 1 Bags, 34.9ms\n",
            "Speed: 5.2ms preprocess, 34.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587868503.jpg: 640x480 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 5.9ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589149605.jpg: 640x384 1 Bags, 24.1ms\n",
            "Speed: 4.1ms preprocess, 24.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36484591591.jpg: 608x640 (no detections), 30.9ms\n",
            "Speed: 6.2ms preprocess, 30.9ms inference, 0.8ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455319131.jpg: 640x480 1 Bags, 26.2ms\n",
            "Speed: 4.3ms preprocess, 26.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485587970.jpg: 640x384 1 Tables, 23.1ms\n",
            "Speed: 2.4ms preprocess, 23.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37593055080.jpg: 640x512 1 Bags, 27.3ms\n",
            "Speed: 4.0ms preprocess, 27.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29611366935.jpg: 640x480 1 Chairs, 1 Tables, 24.9ms\n",
            "Speed: 4.0ms preprocess, 24.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29647305242.jpg: 640x288 1 Tables, 20.4ms\n",
            "Speed: 3.6ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/37593264922.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 8.3ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588972319.jpg: 640x640 1 Bags, 33.1ms\n",
            "Speed: 6.0ms preprocess, 33.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462452167.jpg: 640x480 2 Clothes_baby_girlss, 26.1ms\n",
            "Speed: 4.7ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598567409.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.3ms preprocess, 23.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592790984.jpg: 320x640 (no detections), 23.5ms\n",
            "Speed: 5.1ms preprocess, 23.5ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456736129.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 7.0ms preprocess, 24.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590549703.jpg: 640x480 (no detections), 33.3ms\n",
            "Speed: 7.1ms preprocess, 33.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465271415.jpg: 640x384 1 Bags, 34.1ms\n",
            "Speed: 7.9ms preprocess, 34.1ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36469646439.jpg: 640x640 (no detections), 30.8ms\n",
            "Speed: 5.9ms preprocess, 30.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485731355.jpg: 640x480 (no detections), 26.5ms\n",
            "Speed: 7.4ms preprocess, 26.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460744747.jpg: 640x480 (no detections), 26.8ms\n",
            "Speed: 5.0ms preprocess, 26.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29624341272.jpg: 640x480 1 Tables, 22.7ms\n",
            "Speed: 10.0ms preprocess, 22.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588429663.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 6.1ms preprocess, 24.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29611076522.jpg: 640x480 1 Tables, 23.2ms\n",
            "Speed: 9.2ms preprocess, 23.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29626849722.jpg: 448x640 3 Chairss, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29623658540.jpg: 480x640 1 Chairs, 26.2ms\n",
            "Speed: 4.7ms preprocess, 26.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459786979.jpg: 640x480 (no detections), 24.3ms\n",
            "Speed: 2.7ms preprocess, 24.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29648737881.jpg: 640x448 2 Chairss, 1 Tables, 24.8ms\n",
            "Speed: 9.9ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/29460617877.jpg: 640x384 2 Chairss, 20.9ms\n",
            "Speed: 3.5ms preprocess, 20.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29622717276.jpg: 480x640 1 Tables, 29.4ms\n",
            "Speed: 6.4ms preprocess, 29.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593479559.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 3.1ms preprocess, 23.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465818884.jpg: 640x512 1 Bags, 26.1ms\n",
            "Speed: 4.4ms preprocess, 26.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36464207809.jpg: 640x480 (no detections), 26.0ms\n",
            "Speed: 2.7ms preprocess, 26.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466467969.jpg: 640x480 1 Clothes_baby_girls, 39.0ms\n",
            "Speed: 4.9ms preprocess, 39.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464450167.jpg: 640x480 1 Clothes_baby_girls, 28.4ms\n",
            "Speed: 5.5ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484223990.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 4.3ms preprocess, 25.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592772458.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.4ms preprocess, 23.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460398817.jpg: 640x480 1 Bags, 22.8ms\n",
            "Speed: 4.3ms preprocess, 22.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470906633.jpg: 640x320 (no detections), 21.4ms\n",
            "Speed: 3.0ms preprocess, 21.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29609151168.jpg: 640x288 1 Tables, 31.5ms\n",
            "Speed: 5.8ms preprocess, 31.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36465147985.jpg: 384x640 1 Clothes_baby_girls, 20.1ms\n",
            "Speed: 4.0ms preprocess, 20.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464586258.jpg: 640x480 (no detections), 29.1ms\n",
            "Speed: 3.1ms preprocess, 29.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464074870.jpg: 640x512 1 Bags, 25.5ms\n",
            "Speed: 7.1ms preprocess, 25.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36482650098.jpg: 640x384 1 Bags, 23.1ms\n",
            "Speed: 7.3ms preprocess, 23.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36484541456.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 4.4ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482105346.jpg: 640x480 1 Bags, 31.6ms\n",
            "Speed: 9.7ms preprocess, 31.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596616882.jpg: 640x480 1 Clothes_baby_girls, 22.7ms\n",
            "Speed: 4.4ms preprocess, 22.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595434871.jpg: 640x384 1 Bags, 23.2ms\n",
            "Speed: 5.3ms preprocess, 23.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36483928279.jpg: 640x384 1 Bags, 19.9ms\n",
            "Speed: 2.5ms preprocess, 19.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36461991834.jpg: 640x480 (no detections), 31.4ms\n",
            "Speed: 6.1ms preprocess, 31.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588392816.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 6.4ms preprocess, 25.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464658179.jpg: 640x640 1 Bags, 34.0ms\n",
            "Speed: 4.0ms preprocess, 34.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36467373146.jpg: 640x480 1 Bags, 29.5ms\n",
            "Speed: 4.8ms preprocess, 29.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476120670.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.6ms preprocess, 23.7ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466970916.jpg: 640x480 1 Clothes_baby_girls, 24.2ms\n",
            "Speed: 2.8ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651051049.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 2.8ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485191496.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.9ms preprocess, 23.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486501592.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 4.4ms preprocess, 23.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482627536.jpg: 640x320 (no detections), 22.1ms\n",
            "Speed: 3.0ms preprocess, 22.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36461677598.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 2.4ms preprocess, 24.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457431154.jpg: 480x640 1 Bags, 27.4ms\n",
            "Speed: 4.0ms preprocess, 27.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36478916549.jpg: 640x384 (no detections), 21.4ms\n",
            "Speed: 3.2ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29619597705.jpg: 640x480 1 Tables, 24.7ms\n",
            "Speed: 3.6ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460614018.jpg: 640x480 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 2.7ms preprocess, 23.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620282423.jpg: 640x448 1 Tables, 24.4ms\n",
            "Speed: 3.8ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36461431088.jpg: 640x576 1 Bags, 31.5ms\n",
            "Speed: 3.0ms preprocess, 31.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/29607226565.jpg: 640x480 1 Tables, 25.0ms\n",
            "Speed: 4.5ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462081986.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 2.8ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587872060.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.1ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484767176.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 2.9ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463585275.jpg: 640x512 2 Clothes_baby_girlss, 25.7ms\n",
            "Speed: 5.8ms preprocess, 25.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36415088509.jpg: 480x640 1 Bags, 24.7ms\n",
            "Speed: 3.4ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36406352684.jpg: 640x480 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 3.9ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29629651233.jpg: 640x320 1 Tables, 21.3ms\n",
            "Speed: 2.9ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37589204212.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.7ms preprocess, 24.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483787882.jpg: 384x640 (no detections), 21.5ms\n",
            "Speed: 2.3ms preprocess, 21.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464570026.jpg: 640x640 1 Bags, 33.9ms\n",
            "Speed: 3.6ms preprocess, 33.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29609516373.jpg: 384x640 1 Tables, 21.1ms\n",
            "Speed: 2.0ms preprocess, 21.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465661554.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458377828.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 2.4ms preprocess, 23.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593767858.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483393444.jpg: 576x640 3 Clothes_baby_girlss, 30.4ms\n",
            "Speed: 4.1ms preprocess, 30.4ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29647408670.jpg: 640x480 1 Chairs, 26.5ms\n",
            "Speed: 2.6ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476219896.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 3.9ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596086850.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 4.2ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588844080.jpg: 512x640 (no detections), 25.9ms\n",
            "Speed: 2.6ms preprocess, 25.9ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460955589.jpg: 640x480 2 Clothes_baby_girlss, 25.0ms\n",
            "Speed: 4.0ms preprocess, 25.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461394517.jpg: 640x480 (no detections), 23.0ms\n",
            "Speed: 4.2ms preprocess, 23.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591640974.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 2.7ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486039757.jpg: 640x480 1 Bags, 23.0ms\n",
            "Speed: 3.5ms preprocess, 23.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588766703.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 2.8ms preprocess, 24.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455333133.jpg: 640x640 1 Bags, 35.0ms\n",
            "Speed: 4.4ms preprocess, 35.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469403279.jpg: 640x416 1 Bags, 24.4ms\n",
            "Speed: 3.7ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36460271583.jpg: 448x640 (no detections), 25.7ms\n",
            "Speed: 3.1ms preprocess, 25.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29645437345.jpg: 480x640 1 Tables, 25.2ms\n",
            "Speed: 2.9ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469553467.jpg: 640x608 1 Bags, 34.3ms\n",
            "Speed: 5.4ms preprocess, 34.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36467670747.jpg: 640x544 (no detections), 32.2ms\n",
            "Speed: 4.6ms preprocess, 32.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36464120475.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 2.4ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29622710510.jpg: 640x384 (no detections), 21.4ms\n",
            "Speed: 3.5ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36460887097.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.3ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36329813647.jpg: 640x544 1 Bags, 31.4ms\n",
            "Speed: 4.3ms preprocess, 31.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37594280592.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458727819.jpg: 640x384 1 Bags, 21.0ms\n",
            "Speed: 3.2ms preprocess, 21.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37588108491.jpg: 640x384 (no detections), 19.4ms\n",
            "Speed: 3.4ms preprocess, 19.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37588481382.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 2.5ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466604685.jpg: 640x544 1 Bags, 31.8ms\n",
            "Speed: 3.0ms preprocess, 31.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36484052090.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 4.2ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29632705843.jpg: 640x640 2 Chairss, 32.8ms\n",
            "Speed: 3.3ms preprocess, 32.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461215709.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 4.4ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29621159816.jpg: 640x640 (no detections), 33.2ms\n",
            "Speed: 3.3ms preprocess, 33.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29633547297.jpg: 640x480 1 Tables, 25.7ms\n",
            "Speed: 4.5ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474308153.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 3.0ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459376210.jpg: 640x480 (no detections), 23.5ms\n",
            "Speed: 3.6ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29648984697.jpg: 480x640 1 Tables, 27.0ms\n",
            "Speed: 4.0ms preprocess, 27.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464811494.jpg: 640x640 1 Bags, 33.2ms\n",
            "Speed: 4.4ms preprocess, 33.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29625323347.jpg: 640x544 (no detections), 33.3ms\n",
            "Speed: 2.9ms preprocess, 33.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36481792571.jpg: 640x640 1 Clothes_baby_girls, 33.7ms\n",
            "Speed: 5.6ms preprocess, 33.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590580037.jpg: 640x480 1 Bags, 26.8ms\n",
            "Speed: 4.0ms preprocess, 26.8ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455373659.jpg: 640x480 1 Clothes_baby_girls, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607968978.jpg: 640x480 1 Tables, 24.3ms\n",
            "Speed: 2.8ms preprocess, 24.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464282892.jpg: 576x640 2 Bagss, 33.1ms\n",
            "Speed: 4.4ms preprocess, 33.1ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459145504.jpg: 544x640 1 Bags, 30.0ms\n",
            "Speed: 3.4ms preprocess, 30.0ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598573012.jpg: 640x480 (no detections), 26.2ms\n",
            "Speed: 2.7ms preprocess, 26.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37585383117.jpg: 480x640 1 Bags, 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29604971487.jpg: 448x640 1 Chairs, 2 Tabless, 26.1ms\n",
            "Speed: 3.1ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29612025868.jpg: 640x512 1 Chairs, 25.3ms\n",
            "Speed: 2.8ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36462735282.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.3ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464088337.jpg: 640x480 (no detections), 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485687064.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 4.5ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484899129.jpg: 640x512 (no detections), 25.5ms\n",
            "Speed: 4.5ms preprocess, 25.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36484448584.jpg: 640x384 (no detections), 21.2ms\n",
            "Speed: 3.7ms preprocess, 21.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36472906703.jpg: 480x640 1 Bags, 25.4ms\n",
            "Speed: 3.3ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464750027.jpg: 640x480 1 Clothes_baby_girls, 23.9ms\n",
            "Speed: 4.0ms preprocess, 23.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29626963227.jpg: 640x320 1 Tables, 20.5ms\n",
            "Speed: 2.8ms preprocess, 20.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37594920951.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 2.6ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593162755.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 3.9ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29643899727.jpg: 640x480 3 Bagss, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459339001.jpg: 640x512 1 Bags, 23.8ms\n",
            "Speed: 4.5ms preprocess, 23.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36463053827.jpg: 640x480 1 Clothes_baby_girls, 25.5ms\n",
            "Speed: 2.4ms preprocess, 25.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464110517.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.2ms preprocess, 23.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459286678.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596801163.jpg: 384x640 1 Bags, 20.4ms\n",
            "Speed: 3.4ms preprocess, 20.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461137555.jpg: 640x384 (no detections), 22.0ms\n",
            "Speed: 3.0ms preprocess, 22.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29609941646.jpg: 640x480 1 Chairs, 1 Tables, 25.0ms\n",
            "Speed: 3.0ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587117692.jpg: 640x512 1 Bags, 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36463987265.jpg: 640x480 1 Bags, 26.1ms\n",
            "Speed: 2.6ms preprocess, 26.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482115978.jpg: 640x640 1 Bags, 32.1ms\n",
            "Speed: 3.4ms preprocess, 32.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458169546.jpg: 640x480 1 Clothes_baby_girls, 25.6ms\n",
            "Speed: 4.3ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594339087.jpg: 640x480 (no detections), 24.6ms\n",
            "Speed: 4.0ms preprocess, 24.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591131244.jpg: 480x640 (no detections), 27.6ms\n",
            "Speed: 3.7ms preprocess, 27.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484926365.jpg: 576x640 1 Clothes_baby_girls, 31.1ms\n",
            "Speed: 5.1ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598487172.jpg: 416x640 (no detections), 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458581038.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 3.6ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29610427107.jpg: 480x640 1 Tables, 25.9ms\n",
            "Speed: 2.5ms preprocess, 25.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485401668.jpg: 640x480 1 Chairs, 24.0ms\n",
            "Speed: 5.0ms preprocess, 24.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464600600.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.5ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460735693.jpg: 640x512 1 Bags, 26.0ms\n",
            "Speed: 3.9ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36471385611.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 4.6ms preprocess, 23.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29630610406.jpg: 480x640 (no detections), 26.5ms\n",
            "Speed: 2.5ms preprocess, 26.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457154623.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455635630.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.9ms preprocess, 24.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465671917.jpg: 640x640 (no detections), 31.9ms\n",
            "Speed: 3.4ms preprocess, 31.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587557176.jpg: 640x480 1 Clothes_baby_girls, 26.7ms\n",
            "Speed: 4.5ms preprocess, 26.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36373139787.jpg: 320x640 1 Bags, 19.9ms\n",
            "Speed: 2.3ms preprocess, 19.9ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37596636769.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467602534.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 2.4ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471987651.jpg: 640x480 1 Clothes_baby_girls, 23.3ms\n",
            "Speed: 4.1ms preprocess, 23.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486270284.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463758784.jpg: 640x480 (no detections), 23.6ms\n",
            "Speed: 2.5ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462273233.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 2.5ms preprocess, 24.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483786574.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.1ms preprocess, 24.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472498855.jpg: 640x448 1 Clothes_baby_girls, 23.9ms\n",
            "Speed: 4.1ms preprocess, 23.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37587907451.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 4.3ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462186191.jpg: 480x640 1 Bags, 25.7ms\n",
            "Speed: 2.6ms preprocess, 25.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29643998955.jpg: 640x480 1 Chairs, 25.5ms\n",
            "Speed: 2.8ms preprocess, 25.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463082193.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.0ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597891798.jpg: 640x640 1 Bags, 34.8ms\n",
            "Speed: 5.6ms preprocess, 34.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486694883.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29624968316.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 6.6ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467062778.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 4.2ms preprocess, 22.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597210172.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456369796.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.0ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469224194.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.0ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597588875.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 3.4ms preprocess, 26.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36475389274.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 4.5ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483904443.jpg: 480x640 1 Clothes_baby_girls, 25.8ms\n",
            "Speed: 2.5ms preprocess, 25.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461843810.jpg: 640x480 (no detections), 25.5ms\n",
            "Speed: 3.7ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29605006770.jpg: 640x288 (no detections), 19.8ms\n",
            "Speed: 2.6ms preprocess, 19.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36468677411.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 4.0ms preprocess, 24.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485039577.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 3.3ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594292483.jpg: 640x640 (no detections), 34.1ms\n",
            "Speed: 6.8ms preprocess, 34.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36482744535.jpg: 640x640 1 Bags, 33.1ms\n",
            "Speed: 5.0ms preprocess, 33.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36487035013.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 4.1ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590375239.jpg: 480x640 (no detections), 25.5ms\n",
            "Speed: 2.4ms preprocess, 25.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29647734593.jpg: 448x640 1 Chairs, 1 Tables, 25.8ms\n",
            "Speed: 2.3ms preprocess, 25.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459727560.jpg: 640x384 1 Clothes_baby_girls, 20.9ms\n",
            "Speed: 2.8ms preprocess, 20.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36467711443.jpg: 640x576 1 Bags, 31.6ms\n",
            "Speed: 4.2ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36471909408.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 2.7ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456573366.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.2ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590174385.jpg: 288x640 1 Bags, 19.0ms\n",
            "Speed: 2.6ms preprocess, 19.0ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479605539.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 3.7ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477805358.jpg: 640x640 1 Bags, 31.9ms\n",
            "Speed: 2.7ms preprocess, 31.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29628154361.jpg: 640x448 1 Chairs, 25.3ms\n",
            "Speed: 3.8ms preprocess, 25.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37592162374.jpg: 640x640 1 Bags, 33.5ms\n",
            "Speed: 4.3ms preprocess, 33.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29646992121.jpg: 640x640 (no detections), 32.4ms\n",
            "Speed: 4.4ms preprocess, 32.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29612720268.jpg: 512x640 1 Tables, 26.8ms\n",
            "Speed: 4.3ms preprocess, 26.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37592192660.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.1ms preprocess, 24.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462924980.jpg: 640x320 1 Clothes_baby_girls, 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29648327928.jpg: 640x480 1 Tables, 23.8ms\n",
            "Speed: 2.4ms preprocess, 23.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480511827.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462961876.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.4ms preprocess, 23.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466128058.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 4.2ms preprocess, 24.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589781659.jpg: 480x640 1 Bags, 26.1ms\n",
            "Speed: 2.5ms preprocess, 26.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29649928656.jpg: 640x384 1 Chairs, 1 Tables, 20.7ms\n",
            "Speed: 3.4ms preprocess, 20.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36482440622.jpg: 640x512 1 Bags, 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36464665163.jpg: 480x640 1 Bags, 28.5ms\n",
            "Speed: 4.1ms preprocess, 28.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466847365.jpg: 640x640 (no detections), 31.4ms\n",
            "Speed: 3.2ms preprocess, 31.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593301985.jpg: 640x480 1 Bags, 26.9ms\n",
            "Speed: 4.2ms preprocess, 26.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455889303.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.1ms preprocess, 23.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29605701190.jpg: 640x480 3 Chairss, 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620714142.jpg: 640x384 1 Chairs, 20.8ms\n",
            "Speed: 3.3ms preprocess, 20.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36478474975.jpg: 640x512 1 Bags, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36464400290.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478384679.jpg: 640x640 1 Clothes_baby_girls, 32.2ms\n",
            "Speed: 3.4ms preprocess, 32.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463328205.jpg: 480x640 2 Clothes_baby_girlss, 25.9ms\n",
            "Speed: 2.6ms preprocess, 25.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36474835212.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461170581.jpg: 480x640 1 Clothes_baby_girls, 25.9ms\n",
            "Speed: 2.5ms preprocess, 25.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587935344.jpg: 640x512 (no detections), 24.2ms\n",
            "Speed: 5.0ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29651633568.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 5.4ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588975801.jpg: 640x320 1 Bags, 20.5ms\n",
            "Speed: 2.9ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36457593387.jpg: 640x640 (no detections), 31.4ms\n",
            "Speed: 4.4ms preprocess, 31.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29607804764.jpg: 640x480 1 Clothes_baby_girls, 26.8ms\n",
            "Speed: 4.1ms preprocess, 26.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473250489.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 2.6ms preprocess, 24.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459641819.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 3.7ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591201324.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462579139.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 4.2ms preprocess, 23.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457050860.jpg: 640x480 1 Clothes_baby_girls, 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471461019.jpg: 640x384 1 Bags, 21.4ms\n",
            "Speed: 3.4ms preprocess, 21.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36467678709.jpg: 640x480 1 Bags, 26.0ms\n",
            "Speed: 3.8ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461415674.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 10.0ms preprocess, 23.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466580729.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 2.7ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480168899.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29641749697.jpg: 640x480 1 Tables, 23.2ms\n",
            "Speed: 2.7ms preprocess, 23.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480035692.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 2.5ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463330358.jpg: 640x480 1 Bags, 23.8ms\n",
            "Speed: 2.7ms preprocess, 23.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36373628951.jpg: 640x480 1 Clothes_baby_girls, 24.3ms\n",
            "Speed: 4.1ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475438152.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.1ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466917575.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 2.5ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486221481.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 2.6ms preprocess, 24.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590596331.jpg: 640x480 (no detections), 23.5ms\n",
            "Speed: 5.0ms preprocess, 23.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476274682.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 4.9ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458823995.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461329635.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 4.9ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463159811.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.5ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29652772046.jpg: 384x640 1 Tables, 20.3ms\n",
            "Speed: 3.1ms preprocess, 20.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484571530.jpg: 512x640 1 Bags, 26.4ms\n",
            "Speed: 4.3ms preprocess, 26.4ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29618749178.jpg: 640x640 1 Tables, 35.2ms\n",
            "Speed: 5.8ms preprocess, 35.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484487971.jpg: 480x640 1 Bags, 26.4ms\n",
            "Speed: 4.0ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36467166662.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.2ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471251695.jpg: 480x640 1 Bags, 25.7ms\n",
            "Speed: 4.3ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464917650.jpg: 480x640 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.0ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462551139.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.9ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36465761610.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 3.9ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462213603.jpg: 640x480 1 Bags, 26.5ms\n",
            "Speed: 3.9ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462742593.jpg: 480x640 1 Clothes_baby_girls, 27.4ms\n",
            "Speed: 4.1ms preprocess, 27.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464707006.jpg: 480x640 (no detections), 35.0ms\n",
            "Speed: 2.6ms preprocess, 35.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589300056.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.0ms preprocess, 24.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466286867.jpg: 640x640 (no detections), 32.5ms\n",
            "Speed: 4.8ms preprocess, 32.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457672060.jpg: 640x480 (no detections), 26.1ms\n",
            "Speed: 5.8ms preprocess, 26.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467289744.jpg: 640x480 (no detections), 23.1ms\n",
            "Speed: 6.6ms preprocess, 23.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468741596.jpg: 640x640 1 Bags, 34.4ms\n",
            "Speed: 7.1ms preprocess, 34.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459439265.jpg: 640x480 1 Bags, 27.5ms\n",
            "Speed: 2.2ms preprocess, 27.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29612548045.jpg: 480x640 1 Tables, 26.2ms\n",
            "Speed: 4.2ms preprocess, 26.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462774330.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 8.1ms preprocess, 24.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29642547448.jpg: 640x640 1 Tables, 34.7ms\n",
            "Speed: 3.7ms preprocess, 34.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486631451.jpg: 640x640 1 Bags, 32.1ms\n",
            "Speed: 3.5ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462566279.jpg: 480x640 1 Tables, 27.6ms\n",
            "Speed: 5.1ms preprocess, 27.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36483838680.jpg: 640x384 1 Bags, 22.4ms\n",
            "Speed: 3.4ms preprocess, 22.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36477375033.jpg: 480x640 1 Bags, 26.6ms\n",
            "Speed: 5.0ms preprocess, 26.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456741406.jpg: 576x640 (no detections), 32.2ms\n",
            "Speed: 4.2ms preprocess, 32.2ms inference, 0.8ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465925107.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.3ms preprocess, 24.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588303524.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 4.0ms preprocess, 23.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481235076.jpg: 384x640 1 Bags, 20.8ms\n",
            "Speed: 3.8ms preprocess, 20.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589467264.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 9.2ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473645217.jpg: 640x640 1 Bags, 33.3ms\n",
            "Speed: 5.9ms preprocess, 33.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588641751.jpg: 640x480 1 Bags, 32.0ms\n",
            "Speed: 5.3ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455430499.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 6.3ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461405415.jpg: 640x480 2 Bagss, 25.5ms\n",
            "Speed: 4.3ms preprocess, 25.5ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463888552.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 9.2ms preprocess, 26.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486010867.jpg: 640x512 (no detections), 25.0ms\n",
            "Speed: 4.4ms preprocess, 25.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36478549340.jpg: 640x384 1 Bags, 23.1ms\n",
            "Speed: 5.4ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37588663940.jpg: 640x384 (no detections), 19.3ms\n",
            "Speed: 3.2ms preprocess, 19.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36485589304.jpg: 640x480 1 Clothes_baby_girls, 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483213828.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.9ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467306543.jpg: 640x448 (no detections), 24.3ms\n",
            "Speed: 5.8ms preprocess, 24.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36483026750.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455718897.jpg: 640x480 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 3.9ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595529909.jpg: 640x512 1 Bags, 25.3ms\n",
            "Speed: 4.1ms preprocess, 25.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36459165740.jpg: 640x480 (no detections), 27.7ms\n",
            "Speed: 4.2ms preprocess, 27.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607862236.jpg: 640x640 1 Tables, 32.7ms\n",
            "Speed: 9.3ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464040889.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 4.8ms preprocess, 26.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462648101.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 5.1ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588347293.jpg: 640x480 (no detections), 23.5ms\n",
            "Speed: 3.9ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591333903.jpg: 640x640 1 Bags, 37.4ms\n",
            "Speed: 4.0ms preprocess, 37.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36442270470.jpg: 480x640 1 Bags, 27.3ms\n",
            "Speed: 3.7ms preprocess, 27.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456188432.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 6.4ms preprocess, 24.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595162821.jpg: 640x320 (no detections), 26.3ms\n",
            "Speed: 2.9ms preprocess, 26.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37596613437.jpg: 640x480 (no detections), 27.7ms\n",
            "Speed: 4.0ms preprocess, 27.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481580990.jpg: 512x640 (no detections), 29.3ms\n",
            "Speed: 5.9ms preprocess, 29.3ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595600686.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 3.0ms preprocess, 25.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459880726.jpg: 640x480 (no detections), 23.5ms\n",
            "Speed: 4.2ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588514946.jpg: 640x480 (no detections), 22.9ms\n",
            "Speed: 4.0ms preprocess, 22.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587801889.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.2ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468828367.jpg: 640x480 1 Clothes_baby_girls, 22.8ms\n",
            "Speed: 4.1ms preprocess, 22.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29618372572.jpg: 512x640 1 Tables, 26.4ms\n",
            "Speed: 7.2ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597572882.jpg: 480x640 1 Bags, 27.9ms\n",
            "Speed: 2.8ms preprocess, 27.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29321720506.jpg: 640x480 1 Tables, 24.4ms\n",
            "Speed: 5.1ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596776313.jpg: 640x608 1 Bags, 35.2ms\n",
            "Speed: 5.7ms preprocess, 35.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36477772111.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 7.9ms preprocess, 24.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29640738474.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 6.4ms preprocess, 23.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466448262.jpg: 480x640 1 Bags, 27.1ms\n",
            "Speed: 4.1ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29623377235.jpg: 640x480 1 Tables, 24.8ms\n",
            "Speed: 5.0ms preprocess, 24.8ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460816404.jpg: 480x640 1 Bags, 28.2ms\n",
            "Speed: 2.5ms preprocess, 28.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29642675866.jpg: 480x640 1 Chairs, 1 Tables, 25.4ms\n",
            "Speed: 6.6ms preprocess, 25.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29615830026.jpg: 640x288 1 Chairs, 20.8ms\n",
            "Speed: 3.7ms preprocess, 20.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36485205220.jpg: 480x640 (no detections), 29.9ms\n",
            "Speed: 5.1ms preprocess, 29.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462704189.jpg: 480x640 1 Bags, 24.0ms\n",
            "Speed: 5.8ms preprocess, 24.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598837807.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.3ms preprocess, 24.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456917660.jpg: 640x640 1 Bags, 33.5ms\n",
            "Speed: 4.7ms preprocess, 33.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29619106301.jpg: 640x480 1 Chairs, 25.4ms\n",
            "Speed: 4.8ms preprocess, 25.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462452291.jpg: 480x640 (no detections), 26.4ms\n",
            "Speed: 4.1ms preprocess, 26.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462946665.jpg: 480x640 (no detections), 24.7ms\n",
            "Speed: 4.0ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29621319020.jpg: 640x480 1 Tables, 28.6ms\n",
            "Speed: 8.5ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481686135.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 2.8ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590276863.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29636242991.jpg: 640x320 1 Chairs, 20.7ms\n",
            "Speed: 2.7ms preprocess, 20.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36463153162.jpg: 640x480 1 Bags, 28.8ms\n",
            "Speed: 4.4ms preprocess, 28.8ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476415329.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587513504.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 4.0ms preprocess, 25.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461558349.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 4.3ms preprocess, 22.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455910111.jpg: 640x480 1 Clothes_baby_girls, 32.8ms\n",
            "Speed: 5.1ms preprocess, 32.8ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589622246.jpg: 640x512 (no detections), 39.6ms\n",
            "Speed: 7.2ms preprocess, 39.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36464431017.jpg: 640x480 1 Clothes_baby_girls, 29.9ms\n",
            "Speed: 8.1ms preprocess, 29.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457337783.jpg: 544x640 1 Bags, 30.8ms\n",
            "Speed: 11.0ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458352824.jpg: 640x480 1 Clothes_baby_girls, 26.6ms\n",
            "Speed: 6.0ms preprocess, 26.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587316683.jpg: 640x480 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 4.6ms preprocess, 24.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29610990324.jpg: 640x480 1 Tables, 22.9ms\n",
            "Speed: 4.7ms preprocess, 22.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596500238.jpg: 480x640 (no detections), 25.8ms\n",
            "Speed: 4.6ms preprocess, 25.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461329575.jpg: 480x640 3 Bagss, 24.6ms\n",
            "Speed: 4.3ms preprocess, 24.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461920067.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 4.4ms preprocess, 25.2ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456919622.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 4.3ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36477619422.jpg: 640x640 1 Bags, 32.9ms\n",
            "Speed: 5.8ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37597818319.jpg: 480x640 (no detections), 26.9ms\n",
            "Speed: 4.4ms preprocess, 26.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486392646.jpg: 480x640 1 Bags, 59.0ms\n",
            "Speed: 4.1ms preprocess, 59.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587439984.jpg: 480x640 1 Clothes_baby_girls, 26.3ms\n",
            "Speed: 4.2ms preprocess, 26.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36421050070.jpg: 640x480 1 Bags, 25.3ms\n",
            "Speed: 5.7ms preprocess, 25.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458493625.jpg: 640x480 1 Clothes_baby_girls, 23.2ms\n",
            "Speed: 5.9ms preprocess, 23.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460643034.jpg: 640x480 1 Bags, 30.4ms\n",
            "Speed: 6.7ms preprocess, 30.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596495515.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 4.7ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29617246499.jpg: 640x480 1 Tables, 23.6ms\n",
            "Speed: 5.4ms preprocess, 23.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485324726.jpg: 640x480 (no detections), 29.3ms\n",
            "Speed: 4.5ms preprocess, 29.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595348855.jpg: 640x416 1 Bags, 23.5ms\n",
            "Speed: 5.5ms preprocess, 23.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36473021258.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.5ms preprocess, 24.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588948543.jpg: 640x640 (no detections), 32.2ms\n",
            "Speed: 6.2ms preprocess, 32.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36480910653.jpg: 640x384 1 Bags, 23.1ms\n",
            "Speed: 3.5ms preprocess, 23.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36465373686.jpg: 512x640 1 Bags, 26.7ms\n",
            "Speed: 4.9ms preprocess, 26.7ms inference, 2.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462432468.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587628488.jpg: 640x512 1 Clothes_baby_girls, 29.1ms\n",
            "Speed: 4.5ms preprocess, 29.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36474823145.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 4.2ms preprocess, 24.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468750772.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 4.4ms preprocess, 22.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464583595.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 3.3ms preprocess, 24.7ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588653443.jpg: 480x640 1 Bags, 30.2ms\n",
            "Speed: 3.9ms preprocess, 30.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486774977.jpg: 640x480 (no detections), 25.7ms\n",
            "Speed: 4.4ms preprocess, 25.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594269245.jpg: 640x512 (no detections), 26.7ms\n",
            "Speed: 4.5ms preprocess, 26.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29636562530.jpg: 640x640 1 Chairs, 1 Tables, 33.4ms\n",
            "Speed: 5.3ms preprocess, 33.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461933884.jpg: 640x480 (no detections), 25.8ms\n",
            "Speed: 2.7ms preprocess, 25.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461989581.jpg: 640x480 1 Clothes_baby_girls, 23.5ms\n",
            "Speed: 4.3ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463533808.jpg: 640x480 (no detections), 24.4ms\n",
            "Speed: 2.4ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468506200.jpg: 480x640 (no detections), 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37592223363.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.2ms preprocess, 24.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463341270.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 2.7ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480341310.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 2.6ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485546961.jpg: 640x416 (no detections), 24.0ms\n",
            "Speed: 3.3ms preprocess, 24.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/29607779355.jpg: 640x480 1 Chairs, 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461098871.jpg: 640x480 1 Clothes_baby_girls, 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486742975.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 2.7ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36469859113.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 3.3ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588706091.jpg: 480x640 1 Bags, 26.8ms\n",
            "Speed: 3.9ms preprocess, 26.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460630128.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.1ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36455895007.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 3.7ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463213915.jpg: 640x288 1 Bags, 20.4ms\n",
            "Speed: 3.2ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36456566552.jpg: 640x480 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589675067.jpg: 640x320 1 Bags, 20.1ms\n",
            "Speed: 3.2ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/37588837802.jpg: 384x640 1 Bags, 20.3ms\n",
            "Speed: 3.3ms preprocess, 20.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29623550889.jpg: 640x480 1 Tables, 23.0ms\n",
            "Speed: 8.4ms preprocess, 23.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587976021.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592141808.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 3.0ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/28006459392.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 3.6ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462242344.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 2.7ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29624733784.jpg: 640x480 1 Tables, 23.8ms\n",
            "Speed: 3.0ms preprocess, 23.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591616442.jpg: 640x640 1 Clothes_baby_girls, 32.3ms\n",
            "Speed: 3.3ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462498678.jpg: 608x640 1 Bags, 32.6ms\n",
            "Speed: 3.2ms preprocess, 32.6ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29650774407.jpg: 640x480 (no detections), 26.4ms\n",
            "Speed: 4.3ms preprocess, 26.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483451001.jpg: 480x640 (no detections), 26.9ms\n",
            "Speed: 2.4ms preprocess, 26.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472680664.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.0ms preprocess, 24.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593953296.jpg: 384x640 (no detections), 20.6ms\n",
            "Speed: 1.9ms preprocess, 20.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486912274.jpg: 480x640 (no detections), 26.3ms\n",
            "Speed: 2.4ms preprocess, 26.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460392680.jpg: 640x480 1 Bags, 25.9ms\n",
            "Speed: 2.6ms preprocess, 25.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476006375.jpg: 640x384 1 Bags, 21.9ms\n",
            "Speed: 3.4ms preprocess, 21.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36486139085.jpg: 480x640 1 Tables, 24.9ms\n",
            "Speed: 3.5ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458160552.jpg: 640x320 1 Bags, 21.1ms\n",
            "Speed: 3.2ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29614008655.jpg: 576x640 1 Tables, 31.0ms\n",
            "Speed: 4.6ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37593352766.jpg: 640x512 1 Bags, 26.4ms\n",
            "Speed: 4.5ms preprocess, 26.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36457850812.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 4.3ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597122906.jpg: 640x512 1 Bags, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36459970599.jpg: 640x288 1 Bags, 22.0ms\n",
            "Speed: 2.7ms preprocess, 22.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36468738615.jpg: 480x640 1 Bags, 27.7ms\n",
            "Speed: 3.3ms preprocess, 27.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36457894458.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 3.1ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461792946.jpg: 640x512 1 Bags, 25.1ms\n",
            "Speed: 3.5ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37588381636.jpg: 640x480 (no detections), 25.9ms\n",
            "Speed: 3.1ms preprocess, 25.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593959448.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 3.4ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589678952.jpg: 640x640 (no detections), 32.6ms\n",
            "Speed: 5.6ms preprocess, 32.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29612205232.jpg: 640x480 1 Chairs, 1 Tables, 26.0ms\n",
            "Speed: 4.1ms preprocess, 26.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486809543.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 2.8ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482349610.jpg: 640x480 (no detections), 24.4ms\n",
            "Speed: 4.3ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458328692.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.1ms preprocess, 23.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29605298637.jpg: 640x480 2 Chairss, 23.8ms\n",
            "Speed: 2.6ms preprocess, 23.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467750375.jpg: 640x480 1 Clothes_baby_girls, 24.6ms\n",
            "Speed: 3.2ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484746173.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.9ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588469693.jpg: 640x544 1 Bags, 31.8ms\n",
            "Speed: 4.8ms preprocess, 31.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37594098612.jpg: 480x640 1 Bags, 26.2ms\n",
            "Speed: 2.6ms preprocess, 26.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36477583905.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 3.0ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29627591718.jpg: 640x480 1 Chairs, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457727919.jpg: 640x480 (no detections), 23.3ms\n",
            "Speed: 4.2ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595444969.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469264173.jpg: 640x384 (no detections), 21.7ms\n",
            "Speed: 3.4ms preprocess, 21.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36475545023.jpg: 640x384 1 Bags, 20.6ms\n",
            "Speed: 3.4ms preprocess, 20.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29650430383.jpg: 640x448 1 Chairs, 24.6ms\n",
            "Speed: 4.1ms preprocess, 24.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36457351933.jpg: 640x480 1 Clothes_baby_girls, 25.1ms\n",
            "Speed: 4.4ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590454894.jpg: 640x384 1 Bags, 20.8ms\n",
            "Speed: 3.4ms preprocess, 20.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37596247373.jpg: 640x480 (no detections), 23.6ms\n",
            "Speed: 2.8ms preprocess, 23.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462584742.jpg: 640x384 (no detections), 22.2ms\n",
            "Speed: 2.2ms preprocess, 22.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462487831.jpg: 480x640 2 Bagss, 27.5ms\n",
            "Speed: 4.0ms preprocess, 27.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36467652501.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 2.5ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37599195144.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 2.4ms preprocess, 24.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587956794.jpg: 288x640 1 Clothes_baby_girls, 19.2ms\n",
            "Speed: 1.6ms preprocess, 19.2ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464278809.jpg: 640x480 (no detections), 23.0ms\n",
            "Speed: 2.6ms preprocess, 23.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597834959.jpg: 640x480 1 Bags, 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 4.8ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470961201.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.5ms preprocess, 24.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29620775825.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 3.3ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598225496.jpg: 640x608 1 Bags, 33.2ms\n",
            "Speed: 4.8ms preprocess, 33.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/36469171941.jpg: 480x640 1 Bags, 26.4ms\n",
            "Speed: 4.1ms preprocess, 26.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464568505.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.8ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29637088280.jpg: 480x640 1 Tables, 28.3ms\n",
            "Speed: 3.3ms preprocess, 28.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36464236798.jpg: 640x448 1 Bags, 23.2ms\n",
            "Speed: 5.2ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36469828741.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 4.7ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462439487.jpg: 640x448 1 Clothes_baby_girls, 23.2ms\n",
            "Speed: 4.3ms preprocess, 23.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/29645946105.jpg: 640x480 1 Tables, 25.7ms\n",
            "Speed: 4.4ms preprocess, 25.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462647038.jpg: 640x480 (no detections), 25.7ms\n",
            "Speed: 2.8ms preprocess, 25.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469469072.jpg: 640x384 (no detections), 20.8ms\n",
            "Speed: 4.2ms preprocess, 20.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36471469224.jpg: 640x640 (no detections), 32.5ms\n",
            "Speed: 5.6ms preprocess, 32.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36483088692.jpg: 640x384 1 Bags, 21.8ms\n",
            "Speed: 3.3ms preprocess, 21.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37598527102.jpg: 640x384 1 Bags, 19.2ms\n",
            "Speed: 3.4ms preprocess, 19.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36477312032.jpg: 640x416 1 Clothes_baby_girls, 23.6ms\n",
            "Speed: 3.0ms preprocess, 23.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/29639373435.jpg: 480x640 1 Tables, 27.6ms\n",
            "Speed: 4.2ms preprocess, 27.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587032738.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.1ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29603323067.jpg: 640x480 1 Tables, 24.2ms\n",
            "Speed: 4.2ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29947684270.jpg: 480x640 1 Bags, 25.3ms\n",
            "Speed: 2.6ms preprocess, 25.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461795916.jpg: 640x640 (no detections), 31.5ms\n",
            "Speed: 5.6ms preprocess, 31.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466708706.jpg: 480x640 1 Bags, 26.1ms\n",
            "Speed: 2.6ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36449167651.jpg: 640x576 1 Clothes_baby_girls, 33.0ms\n",
            "Speed: 4.9ms preprocess, 33.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/37589144425.jpg: 480x640 1 Clothes_baby_girls, 27.2ms\n",
            "Speed: 2.7ms preprocess, 27.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466704013.jpg: 480x640 (no detections), 24.1ms\n",
            "Speed: 3.1ms preprocess, 24.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29636255610.jpg: 480x640 1 Tables, 26.0ms\n",
            "Speed: 4.1ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36486610420.jpg: 640x480 1 Bags, 25.7ms\n",
            "Speed: 2.9ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461651667.jpg: 480x640 1 Bags, 26.0ms\n",
            "Speed: 2.7ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36466069327.jpg: 480x640 (no detections), 24.7ms\n",
            "Speed: 2.6ms preprocess, 24.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589656574.jpg: 640x416 1 Clothes_baby_girls, 24.0ms\n",
            "Speed: 3.3ms preprocess, 24.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36479741010.jpg: 640x640 (no detections), 31.9ms\n",
            "Speed: 4.3ms preprocess, 31.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598040014.jpg: 640x480 (no detections), 27.2ms\n",
            "Speed: 2.9ms preprocess, 27.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470691950.jpg: 640x512 1 Clothes_baby_girls, 25.6ms\n",
            "Speed: 4.6ms preprocess, 25.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29644660419.jpg: 640x480 1 Tables, 25.4ms\n",
            "Speed: 2.7ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463706319.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 5.1ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37586912791.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 4.2ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591120071.jpg: 640x608 (no detections), 32.8ms\n",
            "Speed: 4.4ms preprocess, 32.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/train_data/29632881532.jpg: 640x640 2 Chairss, 35.1ms\n",
            "Speed: 4.6ms preprocess, 35.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37592859587.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.2ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485508912.jpg: 640x640 1 Tables, 35.0ms\n",
            "Speed: 4.1ms preprocess, 35.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29609519697.jpg: 640x640 1 Tables, 34.3ms\n",
            "Speed: 3.4ms preprocess, 34.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29617847996.jpg: 480x640 1 Tables, 26.7ms\n",
            "Speed: 2.6ms preprocess, 26.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36477867128.jpg: 480x640 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 2.5ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36483616551.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 4.2ms preprocess, 25.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486119073.jpg: 480x640 1 Bags, 26.3ms\n",
            "Speed: 4.0ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36479023005.jpg: 640x480 1 Clothes_baby_girls, 25.0ms\n",
            "Speed: 4.2ms preprocess, 25.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473082474.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 3.9ms preprocess, 23.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598605147.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.2ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474716367.jpg: 640x512 1 Bags, 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37595002299.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.2ms preprocess, 24.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461619214.jpg: 480x640 1 Bags, 26.4ms\n",
            "Speed: 2.5ms preprocess, 26.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472649736.jpg: 640x480 1 Clothes_baby_girls, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485457400.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 2.5ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459120273.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.2ms preprocess, 24.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29647802480.jpg: 640x544 1 Chairs, 30.4ms\n",
            "Speed: 2.7ms preprocess, 30.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/29608551318.jpg: 640x480 2 Chairss, 1 Tables, 25.0ms\n",
            "Speed: 3.6ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456208621.jpg: 384x640 (no detections), 20.8ms\n",
            "Speed: 2.1ms preprocess, 20.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29608547523.jpg: 480x640 1 Tables, 25.2ms\n",
            "Speed: 2.5ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36481576843.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 3.2ms preprocess, 24.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459914489.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 4.1ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477401218.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 2.5ms preprocess, 23.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37599182643.jpg: 640x640 1 Bags, 34.5ms\n",
            "Speed: 3.3ms preprocess, 34.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36163126322.jpg: 640x512 1 Bags, 26.1ms\n",
            "Speed: 4.3ms preprocess, 26.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36477168414.jpg: 640x480 1 Bags, 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593784236.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 4.4ms preprocess, 23.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460654898.jpg: 480x640 (no detections), 25.7ms\n",
            "Speed: 2.6ms preprocess, 25.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36477582435.jpg: 640x480 1 Clothes_baby_girls, 25.4ms\n",
            "Speed: 4.2ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466139186.jpg: 480x640 1 Bags, 29.1ms\n",
            "Speed: 4.0ms preprocess, 29.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36458940494.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.3ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460159591.jpg: 640x480 1 Clothes_baby_girls, 22.9ms\n",
            "Speed: 3.3ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464805232.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.0ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588603811.jpg: 480x640 (no detections), 25.6ms\n",
            "Speed: 2.4ms preprocess, 25.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29644294578.jpg: 640x480 1 Tables, 25.4ms\n",
            "Speed: 4.9ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457307822.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 3.5ms preprocess, 23.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37587195664.jpg: 640x640 (no detections), 34.9ms\n",
            "Speed: 4.9ms preprocess, 34.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465333938.jpg: 640x480 1 Clothes_baby_girls, 28.6ms\n",
            "Speed: 3.5ms preprocess, 28.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475273371.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.3ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29648302691.jpg: 640x384 (no detections), 21.4ms\n",
            "Speed: 3.8ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36467119032.jpg: 640x384 1 Bags, 21.7ms\n",
            "Speed: 2.8ms preprocess, 21.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29617428057.jpg: 640x480 1 Tables, 24.4ms\n",
            "Speed: 4.0ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466062030.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 3.4ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29621447086.jpg: 640x480 1 Tables, 24.0ms\n",
            "Speed: 4.1ms preprocess, 24.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458531204.jpg: 480x640 1 Bags, 25.6ms\n",
            "Speed: 3.0ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460362879.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 5.2ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36472251986.jpg: 480x640 (no detections), 26.2ms\n",
            "Speed: 2.7ms preprocess, 26.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29605541643.jpg: 480x640 1 Tables, 25.1ms\n",
            "Speed: 4.0ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461592375.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 2.8ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598526221.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 2.5ms preprocess, 23.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593075221.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.0ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459923811.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 3.6ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466007855.jpg: 480x640 1 Bags, 27.0ms\n",
            "Speed: 2.4ms preprocess, 27.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29621062791.jpg: 640x640 1 Chairs, 34.9ms\n",
            "Speed: 5.5ms preprocess, 34.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36463188116.jpg: 640x480 1 Clothes_baby_girls, 27.3ms\n",
            "Speed: 3.9ms preprocess, 27.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592311156.jpg: 640x320 1 Clothes_baby_girls, 20.4ms\n",
            "Speed: 2.8ms preprocess, 20.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36485744512.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 4.3ms preprocess, 24.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29624659685.jpg: 640x384 (no detections), 21.4ms\n",
            "Speed: 3.3ms preprocess, 21.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36459073852.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 3.6ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29647656734.jpg: 640x480 1 Tables, 23.6ms\n",
            "Speed: 3.2ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482439541.jpg: 640x384 (no detections), 24.1ms\n",
            "Speed: 2.2ms preprocess, 24.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36466703072.jpg: 640x640 1 Bags, 4 Clothes_baby_girlss, 31.8ms\n",
            "Speed: 4.1ms preprocess, 31.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472466445.jpg: 640x320 1 Clothes_baby_girls, 21.0ms\n",
            "Speed: 2.5ms preprocess, 21.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36463249405.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29648412072.jpg: 640x448 (no detections), 25.4ms\n",
            "Speed: 3.0ms preprocess, 25.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37589414390.jpg: 480x640 1 Bags, 27.1ms\n",
            "Speed: 4.1ms preprocess, 27.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37594891189.jpg: 640x480 1 Tables, 37.5ms\n",
            "Speed: 5.4ms preprocess, 37.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463618900.jpg: 640x544 1 Bags, 31.9ms\n",
            "Speed: 4.9ms preprocess, 31.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37589033275.jpg: 640x448 1 Bags, 24.8ms\n",
            "Speed: 3.8ms preprocess, 24.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/37594961290.jpg: 640x480 (no detections), 25.4ms\n",
            "Speed: 4.4ms preprocess, 25.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471098958.jpg: 640x480 1 Clothes_baby_girls, 23.0ms\n",
            "Speed: 3.8ms preprocess, 23.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29646527035.jpg: 640x480 1 Tables, 25.2ms\n",
            "Speed: 4.1ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29608887751.jpg: 640x448 1 Tables, 24.6ms\n",
            "Speed: 3.2ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/train_data/36483364573.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 4.2ms preprocess, 24.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29645913948.jpg: 480x640 1 Tables, 25.5ms\n",
            "Speed: 2.5ms preprocess, 25.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36484138963.jpg: 640x416 (no detections), 24.2ms\n",
            "Speed: 3.7ms preprocess, 24.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "image 1/1 /content/train_data/36464506580.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 4.3ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462662441.jpg: 480x640 1 Clothes_baby_girls, 25.9ms\n",
            "Speed: 4.2ms preprocess, 25.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36459855857.jpg: 640x480 (no detections), 25.0ms\n",
            "Speed: 2.5ms preprocess, 25.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29637969615.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.5ms preprocess, 24.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483011535.jpg: 640x320 1 Bags, 20.1ms\n",
            "Speed: 2.8ms preprocess, 20.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36459370421.jpg: 640x480 1 Clothes_baby_girls, 23.4ms\n",
            "Speed: 4.1ms preprocess, 23.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597866298.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.2ms preprocess, 23.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460762881.jpg: 640x480 1 Bags, 24.6ms\n",
            "Speed: 3.9ms preprocess, 24.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588008740.jpg: 640x480 1 Tables, 23.9ms\n",
            "Speed: 3.9ms preprocess, 23.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467002185.jpg: 640x544 (no detections), 30.2ms\n",
            "Speed: 5.4ms preprocess, 30.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36477840300.jpg: 640x480 1 Bags, 26.7ms\n",
            "Speed: 3.9ms preprocess, 26.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589874498.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 2.6ms preprocess, 23.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463152334.jpg: 640x480 (no detections), 25.6ms\n",
            "Speed: 4.4ms preprocess, 25.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29644514104.jpg: 320x640 1 Tables, 20.6ms\n",
            "Speed: 3.9ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29615741144.jpg: 640x480 1 Tables, 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478763434.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.4ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36473301552.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.2ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461070702.jpg: 640x512 (no detections), 26.0ms\n",
            "Speed: 5.1ms preprocess, 26.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36474628143.jpg: 480x640 (no detections), 28.1ms\n",
            "Speed: 6.0ms preprocess, 28.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472928529.jpg: 640x480 1 Bags, 26.3ms\n",
            "Speed: 4.8ms preprocess, 26.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598284628.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.2ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456358798.jpg: 640x480 1 Clothes_baby_girls, 23.1ms\n",
            "Speed: 3.9ms preprocess, 23.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459468553.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 4.2ms preprocess, 24.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651838167.jpg: 640x480 1 Tables, 24.2ms\n",
            "Speed: 3.9ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36475427703.jpg: 640x512 1 Clothes_baby_girls, 25.0ms\n",
            "Speed: 6.8ms preprocess, 25.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36458542059.jpg: 640x576 1 Bags, 31.8ms\n",
            "Speed: 4.8ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36461897411.jpg: 480x640 (no detections), 27.8ms\n",
            "Speed: 3.9ms preprocess, 27.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595739118.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 6.1ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461776896.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 6.0ms preprocess, 24.3ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461342680.jpg: 640x480 (no detections), 23.7ms\n",
            "Speed: 5.9ms preprocess, 23.7ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479372516.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 5.0ms preprocess, 24.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463100784.jpg: 448x640 1 Bags, 26.5ms\n",
            "Speed: 3.8ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36444017532.jpg: 640x640 1 Bags, 30.8ms\n",
            "Speed: 5.5ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595171901.jpg: 640x288 (no detections), 20.5ms\n",
            "Speed: 5.6ms preprocess, 20.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36467085108.jpg: 640x480 1 Bags, 25.0ms\n",
            "Speed: 7.3ms preprocess, 25.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470565970.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 5.2ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484480009.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 2.9ms preprocess, 24.1ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36486547344.jpg: 640x512 1 Bags, 25.4ms\n",
            "Speed: 4.2ms preprocess, 25.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29638949132.jpg: 640x480 1 Tables, 25.4ms\n",
            "Speed: 4.2ms preprocess, 25.4ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459900455.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 6.1ms preprocess, 23.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464569512.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 2.8ms preprocess, 24.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588706202.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 4.0ms preprocess, 23.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593648163.jpg: 640x480 1 Bags, 23.3ms\n",
            "Speed: 4.3ms preprocess, 23.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456661914.jpg: 384x640 (no detections), 22.2ms\n",
            "Speed: 5.3ms preprocess, 22.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588726239.jpg: 640x512 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 4.1ms preprocess, 24.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36457830721.jpg: 480x640 1 Clothes_baby_girls, 28.1ms\n",
            "Speed: 7.0ms preprocess, 28.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472337054.jpg: 448x640 (no detections), 25.3ms\n",
            "Speed: 9.9ms preprocess, 25.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37587663834.jpg: 640x480 1 Bags, 24.4ms\n",
            "Speed: 5.5ms preprocess, 24.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480767292.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 3.9ms preprocess, 24.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29651283155.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 3.6ms preprocess, 24.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36477244730.jpg: 640x480 (no detections), 24.5ms\n",
            "Speed: 4.6ms preprocess, 24.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596130245.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.3ms preprocess, 24.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479358921.jpg: 448x640 1 Bags, 26.4ms\n",
            "Speed: 5.1ms preprocess, 26.4ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461024087.jpg: 640x480 (no detections), 24.0ms\n",
            "Speed: 7.9ms preprocess, 24.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591683966.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 3.9ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29639999018.jpg: 640x480 1 Tables, 23.2ms\n",
            "Speed: 4.1ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598304757.jpg: 640x512 1 Bags, 25.6ms\n",
            "Speed: 4.2ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37589642656.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29645418701.jpg: 640x288 1 Chairs, 19.8ms\n",
            "Speed: 4.3ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36484657578.jpg: 480x640 (no detections), 25.4ms\n",
            "Speed: 5.0ms preprocess, 25.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461571614.jpg: 640x640 1 Bags, 32.2ms\n",
            "Speed: 5.4ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29616387040.jpg: 320x640 1 Tables, 27.2ms\n",
            "Speed: 2.3ms preprocess, 27.2ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36471885791.jpg: 640x480 (no detections), 24.2ms\n",
            "Speed: 4.0ms preprocess, 24.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36471448449.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 3.9ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589534032.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 2.8ms preprocess, 24.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597902290.jpg: 480x640 1 Bags, 28.4ms\n",
            "Speed: 5.7ms preprocess, 28.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36475884155.jpg: 416x640 1 Bags, 25.1ms\n",
            "Speed: 4.1ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29644431447.jpg: 640x480 (no detections), 24.1ms\n",
            "Speed: 5.6ms preprocess, 24.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36474080937.jpg: 640x512 1 Bags, 26.7ms\n",
            "Speed: 4.1ms preprocess, 26.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29632943816.jpg: 640x480 (no detections), 25.5ms\n",
            "Speed: 6.0ms preprocess, 25.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36460083435.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 5.7ms preprocess, 24.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37588637157.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 7.0ms preprocess, 24.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607665252.jpg: 640x480 1 Chairs, 23.0ms\n",
            "Speed: 7.0ms preprocess, 23.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29604371063.jpg: 480x640 (no detections), 25.6ms\n",
            "Speed: 4.2ms preprocess, 25.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588629055.jpg: 640x480 (no detections), 25.2ms\n",
            "Speed: 4.0ms preprocess, 25.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461965650.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 3.0ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466630298.jpg: 640x320 1 Clothes_baby_girls, 21.6ms\n",
            "Speed: 2.8ms preprocess, 21.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36462027043.jpg: 640x480 (no detections), 23.8ms\n",
            "Speed: 7.5ms preprocess, 23.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458546261.jpg: 640x480 1 Bags, 22.8ms\n",
            "Speed: 4.9ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483481652.jpg: 640x320 1 Clothes_baby_girls, 22.9ms\n",
            "Speed: 2.7ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29634231417.jpg: 544x640 (no detections), 33.0ms\n",
            "Speed: 3.6ms preprocess, 33.0ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460944701.jpg: 640x480 1 Tables, 26.9ms\n",
            "Speed: 4.1ms preprocess, 26.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37590533816.jpg: 480x640 (no detections), 27.1ms\n",
            "Speed: 4.3ms preprocess, 27.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460674001.jpg: 640x384 1 Clothes_baby_girls, 23.8ms\n",
            "Speed: 3.8ms preprocess, 23.8ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/29613026478.jpg: 640x480 1 Tables, 26.5ms\n",
            "Speed: 5.2ms preprocess, 26.5ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463256108.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 6.8ms preprocess, 23.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480804579.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 7.3ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36478393359.jpg: 480x640 (no detections), 25.4ms\n",
            "Speed: 3.6ms preprocess, 25.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37594580562.jpg: 640x320 (no detections), 22.6ms\n",
            "Speed: 3.3ms preprocess, 22.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/36455717114.jpg: 576x640 1 Bags, 31.8ms\n",
            "Speed: 6.5ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462381668.jpg: 640x640 (no detections), 32.7ms\n",
            "Speed: 9.0ms preprocess, 32.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36485219612.jpg: 640x320 1 Clothes_baby_girls, 21.6ms\n",
            "Speed: 2.8ms preprocess, 21.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 320)\n",
            "\n",
            "image 1/1 /content/train_data/29618159234.jpg: 640x480 1 Tables, 24.1ms\n",
            "Speed: 6.9ms preprocess, 24.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481947642.jpg: 640x480 (no detections), 24.9ms\n",
            "Speed: 3.9ms preprocess, 24.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470983323.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 4.7ms preprocess, 24.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461311967.jpg: 640x576 1 Bags, 31.7ms\n",
            "Speed: 4.9ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36460160933.jpg: 640x480 (no detections), 25.9ms\n",
            "Speed: 3.0ms preprocess, 25.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462303405.jpg: 640x480 1 Clothes_baby_girls, 23.5ms\n",
            "Speed: 4.2ms preprocess, 23.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37594035327.jpg: 640x480 1 Bags, 23.2ms\n",
            "Speed: 4.5ms preprocess, 23.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36479607274.jpg: 640x512 1 Clothes_baby_girls, 26.4ms\n",
            "Speed: 7.0ms preprocess, 26.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/29609795120.jpg: 640x480 1 Chairs, 24.7ms\n",
            "Speed: 4.8ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37598953377.jpg: 640x480 1 Clothes_baby_girls, 24.2ms\n",
            "Speed: 3.9ms preprocess, 24.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591408506.jpg: 640x480 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 5.9ms preprocess, 24.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468190529.jpg: 640x480 (no detections), 26.7ms\n",
            "Speed: 6.0ms preprocess, 26.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459438196.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 5.7ms preprocess, 23.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36459750463.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 5.0ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36482383150.jpg: 640x512 1 Bags, 25.1ms\n",
            "Speed: 4.8ms preprocess, 25.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36465686448.jpg: 640x384 (no detections), 20.9ms\n",
            "Speed: 3.4ms preprocess, 20.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36462152018.jpg: 640x576 1 Bags, 29.9ms\n",
            "Speed: 6.7ms preprocess, 29.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36480186706.jpg: 640x576 1 Clothes_baby_girls, 31.3ms\n",
            "Speed: 5.9ms preprocess, 31.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "image 1/1 /content/train_data/36456260700.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.6ms preprocess, 24.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616777451.jpg: 640x480 1 Tables, 23.7ms\n",
            "Speed: 4.4ms preprocess, 23.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36463315686.jpg: 480x640 1 Bags, 30.8ms\n",
            "Speed: 2.7ms preprocess, 30.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29651631788.jpg: 640x640 2 Tabless, 35.3ms\n",
            "Speed: 5.5ms preprocess, 35.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36468288209.jpg: 512x640 1 Bags, 26.5ms\n",
            "Speed: 4.8ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36462747095.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 4.6ms preprocess, 24.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462974982.jpg: 640x480 1 Bags, 22.9ms\n",
            "Speed: 4.3ms preprocess, 22.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37589241821.jpg: 480x640 1 Bags, 27.9ms\n",
            "Speed: 3.5ms preprocess, 27.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36455719616.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.2ms preprocess, 24.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467690307.jpg: 640x480 1 Bags, 24.0ms\n",
            "Speed: 2.9ms preprocess, 24.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464387954.jpg: 640x640 2 Bagss, 32.3ms\n",
            "Speed: 8.4ms preprocess, 32.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29646765148.jpg: 640x544 2 Chairss, 1 Tables, 33.8ms\n",
            "Speed: 5.1ms preprocess, 33.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/37587338996.jpg: 384x640 (no detections), 24.7ms\n",
            "Speed: 2.9ms preprocess, 24.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472026242.jpg: 640x480 1 Bags, 28.6ms\n",
            "Speed: 4.3ms preprocess, 28.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36484868817.jpg: 480x640 (no detections), 28.1ms\n",
            "Speed: 4.3ms preprocess, 28.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456190576.jpg: 640x480 (no detections), 25.6ms\n",
            "Speed: 5.2ms preprocess, 25.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36485893793.jpg: 640x480 1 Clothes_baby_girls, 24.8ms\n",
            "Speed: 7.2ms preprocess, 24.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36431948473.jpg: 480x640 1 Clothes_baby_girls, 26.9ms\n",
            "Speed: 4.3ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37595372545.jpg: 640x640 1 Bags, 35.1ms\n",
            "Speed: 4.2ms preprocess, 35.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29641825975.jpg: 640x384 1 Bags, 22.2ms\n",
            "Speed: 3.7ms preprocess, 22.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36456852427.jpg: 640x480 1 Bags, 24.9ms\n",
            "Speed: 4.2ms preprocess, 24.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36487006133.jpg: 640x480 1 Bags, 23.5ms\n",
            "Speed: 2.9ms preprocess, 23.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461455162.jpg: 640x480 1 Clothes_baby_girls, 24.4ms\n",
            "Speed: 4.5ms preprocess, 24.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468953423.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 6.1ms preprocess, 23.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37595274286.jpg: 640x480 1 Bags, 28.3ms\n",
            "Speed: 4.4ms preprocess, 28.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607898687.jpg: 384x640 1 Tables, 33.4ms\n",
            "Speed: 3.6ms preprocess, 33.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37590455569.jpg: 640x512 1 Bags, 28.7ms\n",
            "Speed: 4.3ms preprocess, 28.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/37588414751.jpg: 480x640 1 Bags, 28.1ms\n",
            "Speed: 4.6ms preprocess, 28.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36461045180.jpg: 640x544 1 Bags, 30.9ms\n",
            "Speed: 8.6ms preprocess, 30.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/train_data/36464785446.jpg: 640x480 1 Bags, 35.8ms\n",
            "Speed: 4.5ms preprocess, 35.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458811236.jpg: 640x480 1 Clothes_baby_girls, 23.8ms\n",
            "Speed: 10.4ms preprocess, 23.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29614722331.jpg: 384x640 1 Tables, 35.9ms\n",
            "Speed: 6.0ms preprocess, 35.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465981111.jpg: 640x480 1 Bags, 25.1ms\n",
            "Speed: 5.5ms preprocess, 25.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29607380089.jpg: 480x640 1 Tables, 34.6ms\n",
            "Speed: 6.4ms preprocess, 34.6ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29607806911.jpg: 640x288 1 Chairs, 25.3ms\n",
            "Speed: 2.1ms preprocess, 25.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 288)\n",
            "\n",
            "image 1/1 /content/train_data/36455972673.jpg: 640x480 1 Clothes_baby_girls, 24.7ms\n",
            "Speed: 6.7ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36461427512.jpg: 640x480 1 Bags, 23.6ms\n",
            "Speed: 4.1ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37596991436.jpg: 480x640 1 Bags, 26.8ms\n",
            "Speed: 2.5ms preprocess, 26.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29626800202.jpg: 480x640 3 Chairss, 25.6ms\n",
            "Speed: 2.5ms preprocess, 25.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36480632162.jpg: 640x480 2 Bagss, 24.9ms\n",
            "Speed: 2.4ms preprocess, 24.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36468134203.jpg: 640x480 (no detections), 24.7ms\n",
            "Speed: 3.7ms preprocess, 24.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36462962802.jpg: 480x640 1 Bags, 26.0ms\n",
            "Speed: 2.4ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37598224765.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 3.1ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37591920573.jpg: 640x480 1 Bags, 23.7ms\n",
            "Speed: 4.3ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457519279.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 3.1ms preprocess, 26.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460766245.jpg: 640x480 (no detections), 25.3ms\n",
            "Speed: 4.2ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37593412951.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 6.3ms preprocess, 24.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597918169.jpg: 480x640 1 Bags, 26.5ms\n",
            "Speed: 2.5ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589531215.jpg: 640x480 1 Bags, 24.1ms\n",
            "Speed: 2.4ms preprocess, 24.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36456939612.jpg: 512x640 1 Chairs, 27.7ms\n",
            "Speed: 3.9ms preprocess, 27.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460388894.jpg: 640x480 1 Clothes_baby_girls, 24.5ms\n",
            "Speed: 3.0ms preprocess, 24.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592877638.jpg: 640x480 (no detections), 24.3ms\n",
            "Speed: 3.9ms preprocess, 24.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36450850205.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 3.9ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597622252.jpg: 480x640 (no detections), 26.1ms\n",
            "Speed: 2.4ms preprocess, 26.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36472302509.jpg: 480x640 1 Bags, 24.9ms\n",
            "Speed: 3.2ms preprocess, 24.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37599124804.jpg: 640x480 (no detections), 23.9ms\n",
            "Speed: 2.5ms preprocess, 23.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29605275141.jpg: 640x480 1 Chairs, 24.5ms\n",
            "Speed: 3.4ms preprocess, 24.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29652530099.jpg: 640x480 1 Tables, 24.3ms\n",
            "Speed: 3.4ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29604728516.jpg: 448x640 2 Chairss, 1 Tables, 25.4ms\n",
            "Speed: 3.3ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36456059541.jpg: 480x640 1 Clothes_baby_girls, 26.0ms\n",
            "Speed: 2.7ms preprocess, 26.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36460292365.jpg: 640x480 1 Bags, 24.5ms\n",
            "Speed: 2.5ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37597839310.jpg: 640x480 4 Clothes_baby_girlss, 23.5ms\n",
            "Speed: 2.7ms preprocess, 23.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36483832095.jpg: 640x480 1 Bags, 24.3ms\n",
            "Speed: 2.6ms preprocess, 24.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36480535275.jpg: 640x480 (no detections), 23.4ms\n",
            "Speed: 2.8ms preprocess, 23.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29637475050.jpg: 640x480 1 Chairs, 1 Tables, 25.4ms\n",
            "Speed: 3.9ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464688329.jpg: 480x640 1 Bags, 25.8ms\n",
            "Speed: 4.1ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37588706152.jpg: 640x384 1 Clothes_baby_girls, 21.2ms\n",
            "Speed: 3.6ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36478714108.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.5ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36466835352.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 2.5ms preprocess, 23.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457897648.jpg: 480x640 1 Bags, 25.7ms\n",
            "Speed: 4.3ms preprocess, 25.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37589056508.jpg: 640x480 1 Bags, 25.2ms\n",
            "Speed: 4.3ms preprocess, 25.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36457864508.jpg: 640x480 1 Bags, 23.1ms\n",
            "Speed: 5.3ms preprocess, 23.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481984691.jpg: 640x384 (no detections), 21.3ms\n",
            "Speed: 3.5ms preprocess, 21.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/37597956174.jpg: 416x640 1 Clothes_baby_girls, 24.1ms\n",
            "Speed: 2.8ms preprocess, 24.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/train_data/29635677690.jpg: 640x480 1 Chairs, 24.5ms\n",
            "Speed: 2.5ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476262764.jpg: 640x384 1 Bags, 21.1ms\n",
            "Speed: 3.1ms preprocess, 21.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "image 1/1 /content/train_data/36478733903.jpg: 640x480 1 Bags, 23.4ms\n",
            "Speed: 4.2ms preprocess, 23.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/37592637841.jpg: 640x480 1 Bags, 24.7ms\n",
            "Speed: 2.5ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36467352232.jpg: 640x512 1 Bags, 24.5ms\n",
            "Speed: 4.1ms preprocess, 24.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36461475466.jpg: 640x480 (no detections), 25.1ms\n",
            "Speed: 3.5ms preprocess, 25.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36464758567.jpg: 640x512 1 Tables, 25.2ms\n",
            "Speed: 4.5ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 /content/train_data/36463066466.jpg: 640x480 1 Bags, 23.9ms\n",
            "Speed: 4.2ms preprocess, 23.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36390650924.jpg: 640x640 2 Clothes_baby_girlss, 34.3ms\n",
            "Speed: 5.5ms preprocess, 34.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/37594502875.jpg: 640x480 (no detections), 26.2ms\n",
            "Speed: 4.1ms preprocess, 26.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36470948437.jpg: 640x480 1 Clothes_baby_girls, 23.5ms\n",
            "Speed: 4.5ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36458170072.jpg: 640x480 1 Bags, 24.2ms\n",
            "Speed: 4.1ms preprocess, 24.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/29616654883.jpg: 640x480 (no detections), 23.2ms\n",
            "Speed: 4.4ms preprocess, 23.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36476101560.jpg: 640x480 1 Bags, 24.8ms\n",
            "Speed: 3.5ms preprocess, 24.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36469767536.jpg: 640x640 1 Bags, 32.3ms\n",
            "Speed: 4.1ms preprocess, 32.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/train_data/36465373567.jpg: 640x480 (no detections), 24.4ms\n",
            "Speed: 2.6ms preprocess, 24.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/train_data/36481519855.jpg: 640x480 1 Clothes_baby_girls, 24.9ms\n",
            "Speed: 4.1ms preprocess, 24.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Ultralytics 8.3.88 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/runs/detect/Avito_Hack_v4/weights/best.pt, data=/content/dataset_train/one/dataset.yaml, epochs=50, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=Avito_Hack_v5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=0.8, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/Avito_Hack_v5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5585884  ultralytics.nn.modules.head.Detect           [4, [256, 512, 512]]          \n",
            "Model summary: 209 layers, 43,632,924 parameters, 43,632,908 gradients, 165.4 GFLOPs\n",
            "\n",
            "Transferred 110/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/Avito_Hack_v5', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset_train/one/labels... 26097 images, 0 backgrounds, 17 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26097/26097 [01:08<00:00, 379.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/0226fc21-36455397480.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/054dcec8-29609478060.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/2c361020-36455537468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/429e07e5-36455430499.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0013]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/44d77da3-36455523193.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0029]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/54df5495-29615489265.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/59802cc3-36455544135.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/5c107aed-29615858349.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/7986cf2f-36455602280.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/94de3322-36455580468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/a6b57513-36455613018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ce8dca4a-27751638538.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.001]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/d199e7fd-27852669465.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3a8a11d-36455444159.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3c51271-36455653392.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e4a25f32-36455511260.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ece839a8-36429516961.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset_train/one/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 1217, len(boxes) = 28375. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset_train/one/labels.cache... 26097 images, 0 backgrounds, 17 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26097/26097 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/0226fc21-36455397480.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/054dcec8-29609478060.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0015]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/2c361020-36455537468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/429e07e5-36455430499.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0013]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/44d77da3-36455523193.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0029]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/54df5495-29615489265.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/59802cc3-36455544135.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/5c107aed-29615858349.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0026]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/7986cf2f-36455602280.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/94de3322-36455580468.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/a6b57513-36455613018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ce8dca4a-27751638538.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.001]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/d199e7fd-27852669465.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3a8a11d-36455444159.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0006]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e3c51271-36455653392.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0028]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/e4a25f32-36455511260.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0017]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset_train/one/images/ece839a8-36429516961.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0036]\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 1217, len(boxes) = 28375. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/Avito_Hack_v5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/Avito_Hack_v5\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50      9.72G     0.8966      1.137      1.465         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1630/1630 [22:26<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 815/815 [08:36<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      26080      28375      0.889      0.879      0.947      0.868\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50      9.43G     0.5814     0.7472      1.193         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1630/1630 [22:11<00:00,  1.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 815/815 [08:58<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      26080      28375      0.877      0.885      0.946      0.871\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50      9.43G       0.58     0.7326       1.18         39        640:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1416/1630 [19:38<03:03,  1.17it/s]"
          ]
        }
      ]
    }
  ]
}